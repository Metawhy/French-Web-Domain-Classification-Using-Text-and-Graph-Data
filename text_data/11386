   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » L’optimisation bayésienne par l’exemple : à quoi ça sert
   et comment ça marche ? Flux des commentaires Le chemin vers l’omnicanal
   Le demi-cercle (épisode 49 — Cocktail) alternate alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

L’optimisation bayésienne par l’exemple : à quoi ça sert et comment ça marche
?

   Posté le 02/08/2018 par Louis Boutin, Paul De Nonancourt
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   “Si j’ai une valeur y qui est fonction de x, comment faire pour
   déterminer la valeur de x minimisant ou maximisant la valeur de y ?”
   tel est le problème de base du domaine de l’optimisation, qui se
   décline à de très nombreux cas d’usage allant de “comment fixer le prix
   pour maximiser un profit” à “quelle stratégie mon robot doit-il adopter
   pour rester en équilibre”.

   Nous vous proposons dans cet article une introduction aux stratégies
   d’optimisation bayésienne, un sous-domaine regroupant des techniques
   très puissantes pour converger efficacement vers des valeurs optimales
   lorsqu’on fait face à une situation où le nombre d’observations est
   limité par des contraintes de temps ou de matériel.

   Vous souhaitez voir un cas d’usage de ces algorithmes ? Comprendre
   comment ces derniers fonctionnent et comment les implémenter ? Alors
   vous êtes au bon endroit. Cet article prend pour exemple la
   problématique de la configuration automatique des paramètres d’une base
   de données pour illustrer l’optimisation bayésienne. Nous présenterons
   un exemple d’implémentation en Python ainsi que quelques autres cas
   d’usage pour cette approche.

1 – L’optimisation bayésienne au service de la performance d’une base de
données

1.1 – Un exemple concret

   Chez OCTO Technology, nous avons récemment décidé d’explorer des
   questions de configuration automatique du fonctionnement de bases de
   données, un domaine particulièrement large mais avec plusieurs
   problématiques que l’on peut distinguer. L’une d’elles en particulier
   va nous intéresser ici car elle constitue un bon exemple de situation
   où l’optimisation bayésienne se révèle pertinente.

   Imaginons une base de données, pas nécessairement complexe, avec un
   ensemble de paramètres à régler : cache, mémoires partagées, nombre de
   connexions simultanées maximum, etc. Étant donné une charge de travail,
   tel qu’un ensemble de requêtes exécutées sur cette base de données, on
   souhaite déterminer automatiquement les valeurs de paramètres qui nous
   permettront d’obtenir les meilleurs résultats (par exemple, nous
   voudrions compléter l’exécution de notre charge le plus rapidement
   possible). Il existe des règles métier fournissant des pistes pour
   fixer ces paramètres : le site PgTune propose par exemple des
   configurations pour Postgres en fonction du hardware (RAM, CPU). Mais
   ces règles ne sont pas forcément adaptées à notre charge de travail,
   sont rarement optimales et ne s’appuient pas nécessairement sur des
   théories. Pour résumer, on souhaiterait rendre le choix d’une bonne
   configuration le plus automatisé et le plus efficace possible mais il
   est en réalité très difficile de déterminer des règles précises
   régissant ces paramètres.

   Heureusement pour nous, il s’agit finalement d’un problème classique
   d’optimisation pour lequel de nombreuses solutions existent : nous
   voulons trouver rapidement une configuration (c’est-à-dire un ensemble
   de paramètres) maximisant ou minimisant une métrique de performance.
   Pour cela, nous devons tout de même prendre en compte un certain nombre
   de contraintes :
     * Tester une configuration prend du temps car nous devons tester une
       charge dans son intégralité sur notre base de données, nous ne
       pouvons donc pas tester un nombre illimité de configurations pour
       déterminer la meilleure.
     * Le comportement de la performance de la base de données en fonction
       de la configuration est une boîte noire, nous ne pouvons que donner
       une configuration en entrée et observer une performance en sortie.

   on optimise la configuration d'une une base de données en fonction
   d'une charge et une métrique de performance

   Le problème de départ : comment trouver rapidement la configuration
   optimisant la performance, c’est-à-dire ici minimisant le temps
   d’exécution ?

2.2 – Les avantages de l’optimisation bayésienne

   Compte tenu de nos contraintes, il est nécessaire de trouver une
   stratégie d’optimisation pas à pas efficace : nous souhaitons, en un
   minimum d’observations, déterminer la meilleure configuration. En
   consultant la littérature académique, nous avons ainsi identifié deux
   articles explorant également le problème de la configuration
   automatique d’une base de données : iTuned (2009) et Ottertune (2017).
   Les auteurs de ces deux publications proposent des solutions similaires
   basées sur l’optimisation bayésienne afin de rechercher une
   configuration optimale.

   Cette approche séquentielle se trouve être particulièrement adaptée à
   notre problème de par son principe. L’objectif est d’utiliser un petit
   nombre d’observations pour estimer un comportement plus global. En
   exploitant efficacement la connaissance accumulée sur notre fonction
   boîte noire, on espère minimiser le nombre d’observations et converger
   rapidement vers la configuration optimale.

   Étudions maintenant plus en détail le fonctionnement d’un algorithme
   d’optimisation bayésienne avant de passer à une implémentation concrète
   pour répondre à notre problème.

2 – Sous le capot de l’optimisation bayésienne

2.1 – Extraire de l’information à partir d’un nombre limité d’observations

   L’optimisation bayésienne est une approche probabiliste basée sur
   l’inférence bayésienne. En somme, cela veut dire qu’on va chercher à
   exploiter ce qu’on connaît déjà, donc l’ensemble des événements
   précédemment observés, pour inférer la probabilité des événements que
   nous n’avons pas encore observés. Dans le cadre de l’optimisation
   bayésienne, nous partons d’un ensemble d’observations dont nous
   connaissons le résultat et nous déterminons pour chaque valeur ???? en
   dehors de cet ensemble la distribution de probabilité de l’évaluation
   de ???? en ce point ????.


   Dans cet exemple, on cherche à minimiser une fonction qu’on ne connaît
   pas. Comment utiliser ces premières observations pour inférer les
   valeurs lorsque x varie entre -2 et 2 ?


   Mais comment calculer cette distribution de probabilité ? Parmi toutes
   les méthodes existantes, nous retiendrons ici l’une des méthodes
   classiques qui consiste à utiliser les processus gaussiens qui
   généralisent le concept de loi normale aux fonctions. Nous ne
   développerons pas cette méthode un peu complexe. L’important est de
   comprendre que cette approche nous permet de générer pour chaque point
   une distribution de probabilité caractérisée par une moyenne µ (la
   valeur la plus probable) et un écart-type σ (la mesure de la dispersion
   probable de la valeur autour de la moyenne).

   Pour le lecteur souhaitant aller plus loin, cet excellent article de
   blog développe davantage les processus gaussiens sans aller trop loin
   dans les détails mathématiques. La publication originale est ensuite la
   source la plus exhaustive sur le sujet.

   Les processus gaussiens permettent d’inférer la distribution de
   probabilité pour chaque point avec la valeur moyenne estimée µ en
   pointillé, et l’écart-type σ représenté par la zone vert pâle.

   Il est à noter qu’on ne pourra pas représenter cette distribution pour
   tous les points. Nous ne considérons cette distribution que sur un
   ensemble fini de points : c’est notre espace de recherche. Ainsi nous
   aurons, pour chaque point de cet espace de recherche, une valeur
   centrale, la moyenne μ, et une certaine dispersion autour de cette
   moyenne, l’écart-type σ. Ce dernier sera d’autant plus faible que l’on
   sera proche d’un point déjà observé.

   Nous sommes à présent capable d’inférer le comportement de notre
   fonction à partir de nos évaluations. Mais comment choisir quel point
   évaluer : quel point a les meilleures chances d’être un minimum ?

2.2 – Déterminer les points à plus fort potentiel avec la fonction
d’acquisition

   Rappelons-nous notre objectif : minimiser ou maximiser la fonction
   observée, c’est-à-dire trouver le point pour lequel l’évaluation est
   minimisée ou maximisée. Le choix du point utilisé pour l’évaluation
   suivante est donc soumis à un double critère. Nous voulons d’une part
   gagner en connaissance sur le comportement de la fonction et donc
   choisir une zone de l’espace de recherche où l’inconnu est grand :
   c’est l’exploration. D’autre part, nous souhaitons trouver le point qui
   minimise/maximise notre fonction : c’est l’exploitation. Ces deux
   notions sont matérialisées par les indicateurs statistiques cités
   précédemment que sont l’écart-type et la moyenne. Quand l’écart-type
   est grand, c’est que la zone est mal connue et donc intéressante à
   explorer. Quand la moyenne est petite/grande, c’est que la zone
   observée est intéressante pour trouver un minimum/maximum.

   Ce compromis entre exploration et exploitation est exprimé par une
   fonction d’acquisition. Cette fonction associe à chaque point de
   l’espace de recherche un potentiel pour être l’optimal. Il existe
   plusieurs fonctions d’acquisitions, avec des variations subtiles mais
   gardant le même principe directeur : exprimer le potentiel d’un point.
   fonction d'acquisition Upper Confidence Bound

   Upper Confidence Bound est un exemple de fonction d’acquisition, le
   coefficient k exprime le poids donné à l’exploration dans le compromis
   évoqué précédemment µ : moyenne – σ : écart-type


   À chaque étape de notre optimisation, le point choisi pour l’évaluation
   donc est celui qui maximise notre fonction d’exploitation.


   En bleu, notre fonction d’acquisition nous indique les points à plus
   fort potentiel à tester : ils représent le compromis entre exploitation
   (près du minimum) et exploration (dans des zones inconnues)

   Trouver le nouveau point à évaluer implique donc d’évaluer notre
   fonction d’acquisition pour tout notre espace de recherche. Cependant
   ces évaluations seront beaucoup moins coûteuses en comparaison à
   l’évaluation de la fonction observée. La fonction d’acquisition étant
   simple et connue, on peut facilement l’évaluer en un grand nombre de
   points pour trouver le maximum. Le maximum trouvé correspond au
   prochain point à tester et une fois l’observation réalisée, on peut
   actualiser notre optimisation avec cette nouvelle information et
   repartir depuis le début, en répétant jusqu’à converger.

   Pour bien comprendre cela, reprenons notre exemple en voyant après
   plusieurs itérations comment chaque boucle apporte de l’information
   supplémentaire jusqu’à atteindre un minimum. On peut comparer enfin
   l’approximation obtenue à la fonction originale (inconnue à la base) à
   minimiser et vérifier l’efficacité de l’algorithme.

   Le cycle d’optimisation bayésienne à la recherche la valeur donnant le
   résultat minimal sur dix itérations

   Le résultat final après dix itérations avec l’approximation µ(x) faite
   par l’optimisation bayésienne comparé à la “vraie” fonction à
   minimiser, inconnue au départ

   Pour voir cela en pratique, passons à un exemple plus concret !

3 – Et en pratique, comment ça se passe ?

   Maintenant que nous en savons un peu plus sur l’optimisation
   bayésienne, reprenons notre problème initial et voyons comment
   implémenter en Python un algorithme répondant à ce même problème.

   Supposons une situation où nous souhaitons configurer notre base de
   données sur laquelle nous pouvons faire varier deux paramètres param_1
   et param_2. Le but est de déterminer quelles sont les valeurs de ces
   deux paramètres permettant de minimiser le temps pour compléter une
   charge de travail (par exemple un ensemble de requêtes). Les règles
   métier dont nous disposons ne nous permettent pas de déterminer les
   valeurs optimales pour ces paramètres et nous fournissent au mieux des
   bornes dans lesquelles rechercher l’optimum.

3.1 – Une implémentation en Python avec scikit-optimize

   Nous allons ainsi utiliser le package Python scikit-optimize (basé sur
   scikit-learn) pour mettre en place un workflow prenant en entrée un
   intervalle pour nos valeurs de paramètres et cherchant l’ensemble
   donnant les meilleurs résultats. Tout le code du workflow et des
   visualisations est disponible sous forme de notebook Jupyter.

   Commençons par définir notre espace de recherche sous forme de
   dictionnaire :
search_space = {
  'param_1': (0.0, 10.0),
  'param_2': (0.0, 0.0)
}

   Il faut désormais instancier un objet Optimizer, tiré du package
   scikit-optimize, qui va se charger de la majeure partie du travail.
   Nous devons préciser à l’Optimizer l’espace sur lequel il va effectuer
   sa recherche. Comme expliqué précédemment, nous allons utiliser un
   estimateur basé sur les processus gaussiens (ou Gaussian Process,
   “GP”).

   Pour éviter des effets de bord des processus gaussiens survenant
   lorsque l’espace des observations est vide (ou quasi-vide), il est
   aussi préférable de tester les premiers points de manière aléatoire, ce
   que l’on précise par le paramètre n_initial_points.
opt = Optimizer([search_space['param_1'], search_space['param_2']],
                "GP",
                n_initial_points=3)

   Il n’y a maintenant plus qu’à faire tourner la boucle d’optimisation.
   La force de scikit-optimize se situe dans la possibilité de faire
   tourner des workflows relativement asynchrones grâce aux fonctions ask
   et tell. À chaque étape, nous allons demander (ask) quelle est la
   configuration qui présente le meilleur potentiel pour pouvoir la
   tester, c’est-à-dire exécuter notre charge de travail, avant d’obtenir
   un résultat que nous allons ajouter à nos observations (tell). On
   boucle ensuite autant de fois que l’on souhaite ou jusqu’à ce qu’un
   critère de satisfaction soit atteint.
for _ in range(20):
    # Quelle est la prochaine configuration à tester ?
    next_config = opt.ask()  # Renvoie une liste

    # Ici, nous avons une dépendance extérieure puisque nous devons mettre à
    # jour la configuration de la base de données puis tester notre charge.
    # Faisons l'hypothèse que ces interfaces ont été définies ailleurs dans le c
ode.
    database.update_configuration(next_config)
    runtime = database.benchmark(workload)

    opt.tell(next_config, runtime)

   Si tout se passe bien, l’optimiseur convergera ensuite naturellement
   vers un couple de paramètres présentant la meilleure performance.

   Pour se représenter cela, supposons que le temps nécessaire pour
   compléter la charge se comporte de la manière suivante en fonction des
   paramètres param_1 et param_2 :

   Ce comportement est ici hypothétique et l’optimiseur ne le connaît pas.
   Voici comment évolue l’optimisation pour déterminer le minimum à chaque
   itération (pour rappel, le détail est disponible dans ce notebook) :

   L’optimisation bayésienne démontre là tout son intérêt : en un nombre
   d’observations très restreint (de l’ordre de la dizaine), nous sommes
   en mesure de déterminer les valeurs des paramètres pour lesquelles
   notre base de données a les meilleurs résultats. Rapidement,
   l’algorithme converge vers des valeurs autour de (6, 13) en gardant
   tout de même un caractère exploratoire pour s’assurer de ne pas être
   coincé sur un minimum local si le comportement de la base de données
   avait été plus complexe.

3.2 – Gestion du bruit et de la variabilité de l’environnement

   Sur un cas réel, il faut cependant se poser la question du bruit :
   l’optimisation bayésienne est-elle résiliente lorsque les observations
   sont bruitées (dans notre cas, on peut par exemple avoir un temps qui
   varie à cause de perturbations réseaux) ? La réponse est oui, dans une
   certaine mesure. Sur un cas plus compliqué, disponible sur ce notebook,
   on voit qu’un niveau de bruit inférieur à 5% peut perturber
   l’algorithme sans que cela ne soit gênant pour déterminer un minimum
   rapidement. Un autre exemple sur un espace à une dimension démontre
   également cela sur le repository du package scikit-optimize.

   Lorsque le bruit est plus élevé, il est possible de jouer sur des
   paramètres qui peuvent prendre en compte l’incertitude (c’est le cas du
   paramètre alpha de l’estimateur scikit-learn GaussianProcessRegressor).
   Mais lorsque cette variabilité est trop élevée, l’algorithme ne peut
   simplement pas fonctionner correctement. Pour cette raison, il est
   conseillé avant d’utiliser une stratégie d’optimisation bayésienne de
   vérifier que l’environnement est suffisamment stable et que les
   observations sont reproductibles. Après tout, on ne peut pas optimiser
   ce qui varie.

   Des observations trop bruitées ne permettent pas de converger vers un
   minimum : ici, les résultats varient trop pour pouvoir décider quel
   point tester ensuite

3.3 – Application au tuning de modèles de machine learning

   L’exemple précédent d’application de l’optimisation bayésienne est
   relativement précis, mais la stratégie peut facilement se généraliser à
   d’autres applications où l’on cherche un ensemble de paramètres
   maximisant la performance mais que chaque observation est coûteuse.

   C’est notamment le cas de la gestion des hyper-paramètres d’un modèle
   de machine learning, la librairie scikit-optimize est d’ailleurs
   taillée pour ce type d’utilisation. En effet, lorsqu’on utilise un
   modèle de machine learning, il est difficile d’évaluer directement la
   valeur optimale de certains paramètres (par exemple le nombre d’arbres
   et leur profondeur pour un RandomForestClassifier). Utiliser
   l’optimisation bayésienne pour résoudre le problème à notre place est
   tout à fait possible : l’entrée de notre problème correspond aux
   valeurs des hyper-paramètres et la performance à optimiser est une
   métrique au choix de l’utilisateur (précision, AUC, etc.).

   Comment déterminer rapidement les valeurs des hyper-paramètres
   maximisant la métrique de performance ?


   Cette approche permet notamment de gagner en temps d’optimisation en
   réduisant le nombre d’observations par rapport à des algorithmes type
   RandomSearch ou GridSearch qui demandent plus de tests pour arriver à
   des résultats. C’est d’ailleurs l’approche plébiscitée en 2017 par
   Google pour le tuning d’hyper-paramètres qui met même en place un
   système de “hyperparameter tuning as a service” sur Cloud ML.

   Convaincu par l’optimisation bayésienne ? Vous souhaitez optimiser
   rapidement vos modèles de machine learning ? Jetez donc un oeil aux
   solutions prêtes à l’emploi de librairies Python comme scikit-optimize
   ou bayesian-optimization qui présentent toutes les deux des exemples
   d’implémentation appliquée au tuning de modèles ici et ici.

Conclusion

   L’approche bayésienne est donc efficace pour répondre au problème du
   choix de paramètres optimaux lorsque le comportement de la fonction est
   mal connue, que les observations sont coûteuses et qu’une approche
   aléatoire ou brute force n’est pas envisageable. Les algorithmes
   d’optimisation bayésienne se prêtent donc bien à la configuration de
   systèmes d’informations comme une base de données, à condition de bien
   connaître son environnement et sa variabilité !

   Enfin, on a pu récemment voir de grandes entreprises communiquer sur
   des initiatives exploitant ces approches. Des ingénieurs de Twitter par
   exemple, développent un système utilisant l’optimisation bayésienne
   pour configurer automatiquement et en continu les paramètres JVM de
   leurs services. En juin 2018, Facebook a présenté Spiral, une librairie
   destinée à la configuration automatique et temps réel des services
   avec, en préparation, l’utilisation de l’optimisation bayésienne pour
   la configuration de paramètres. Autant d’approches illustrant la force
   de l’optimisation bayésienne lorsqu’on l’applique à la configuration de
   systèmes informatiques.
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans Data Science.

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

   Sécurité

Comment conserver les mots de passe de ses utilisateurs en 2019 ?

   Posté le 02/10/2019 par Fabien Leite
   Mot de passe

   Lorsque vous concevez une application, vous vous posez forcément la
   question de l’authentification et du contrôle d’accès. Pour ça,
   plusieurs méthodes sont disponibles et la première qui vient
   généralement à l’esprit est l’utilisation d’un couple identifiant / mot
   de passe. Dans la mesure du possible, on préfèrera utiliser une
   solution dédiée à l’authentification et au contrôle d’accès : en bref,
   utiliser une solution d’IAM pour gérer ces aspects à votre place. C’est
   généralement plus simple à maintenir et c’est surtout souvent meilleur
   pour l’expérience utilisateur. …
   Lire la suite

   Méthode

Amélioration continue : Comment rester dynamique à mesure que l’équipe
s’agrandit ?

   Posté le 01/10/2019 par Etienne Girot

   Différentes études1 soutiennent qu’une équipe performante est une
   équipe qui est capable de remettre fréquemment en question ses modes de
   fonctionnement afin d’apprendre et s’améliorer en continu. Pour y
   arriver, elle : favorise l'émergence de nouvelles idées a moyen de
   valider ou d’invalider efficacement la pertinence de ces nouvelles
   idées est en mesure d’aligner ses membres derrière les idées retenues
   comme étant pertinentes Or, plus une équipe grandit (aussi bien en
   nombre de membres qu’en temps passé à travailler ensemble) plus elle
   est sujette à…
   Lire la suite

   Méthode innovation

Culture Innov’ : Quel ROI attendu ?

   Posté le 01/10/2019 par Sylvain Fagnent, Matthieu VETTER
   [school.png]

   Après des années et des millions investis sous la menace de la
   disruption, les Directions reviennent à une logique plus ROIste. Ce
   mouvement est sain pour optimiser les ressources rares et donner du
   sens au travail des innovateurs. Le risque est cependant de tuer dans
   l’oeuf des pépites potentielles en pilotant l’innovation comme un
   business opérationnel. En fonction des objectifs, les retours sur
   investissement (ROI) d’une démarche d’innovation seront différents. Et
   dans un monde de plus en plus focalisé sur la rentabilité à court
   terme,…
   Lire la suite

   Data Science

Ouvrir la boîte noire et comprendre les décisions des algorithmes

   Posté le 30/09/2019 par Annabelle Blangero
   [school.png]

   L’usage des algorithmes de traitement de données – de la simple requête
   SQL aux puissants algorithmes de recommandation et de personnalisation
   des géants de la Tech – s’est popularisé ces dernières années,
   notamment pour des utilisateurs traditionnellement hors du domaine IT.
   Cet usage se retrouve dans tous les secteurs (industrie, éducation,
   santé, sécurité, etc.) et tend à déléguer de plus en plus de décisions
   à des systèmes automatisés. Cette appropriation par le plus grand
   nombre rend les naufrages encore plus probables, et l’exemple de
   Cambridge…
   Lire la suite

   Archi & techno

Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos valeurs et le
garant de la vision technique d’OCTO.”

   Posté le 27/09/2019 par Joy Boswell
   [archi.png]

   Chez OCTO depuis plus de 10 ans , Meriem fait partie des personnes
   fondatrices de l’entreprise. Ancienne leadeuse de la tribu Nouvelles
   Architectures de Données, elle est désormais CTO et participe à la
   définition de la vision stratégique et technique d’OCTO. Qui de mieux
   pour nous parler du “tech leading à la OCTO” ?
   Lire la suite

   Data Science

Mise en application de DVC sur un projet de Machine Learning

   Posté le 27/09/2019 par Nicolas TOUSSAINT, Jérémy Bouhi
   [school.png]

   Introduction DVC (Data Version Control) est un package Python qui
   permet de gérer plus facilement ses projets de Data science. Cet outil
   est une extension de Git pour le Machine Learning, comme l’énonce son
   principal contributeur Dmitry Petrov dans cette présentation. DVC est à
   la fois comparable et complémentaire à Git. Il va s’occuper de
   synchroniser vos données et votre code. Il est donc particulièrement
   intéressant dans le cadre d’un projet de Machine Learning où le modèle
   et les données évoluent au fil du développement.…
   Lire la suite

   Archi & techno

BD – Le Déploiement Continu (CD)

   Posté le 26/09/2019 par Aryana Peze
   [archi.png]

   Hello ! Lors de la BD précédente, nous avons abordé le sujet de la CI
   (Intégration Continue). Et impossible de parler de CI sans parler de CD
   (Déploiement Continu)! En thoérie, la CD implique un déploiement
   automatique et quasi-systématique de chaque modification du code sur
   l'environnement de production. Les mises en production sont régulières
   et ne sont plus une source de stresse, et l'environnement de production
   est ainsi toujours à jour. En pratique, c'est un objectif très
   compliqué à atteindre, et pas toujours adapté. (Petite parenthèse…
   Lire la suite

   Agile

D’étudiant à mentor : rencontre avec notre Octo Thomas Le Flohic

   Posté le 24/09/2019 par Céline Audibert
   [agile.png]

   Thomas a fait un véritable parcours “à la OCTO” : après son école
   d’ingé, il rentre en stage au sein de notre tribu VIBE (Virtual
   Immersion and Bot Experience) et rejoint définitivement l’entreprise en
   intégrant le programme Skool. Un de ses profs à l’école était un Octo,
   Fabien. C’est ce qui lui a donné envie de venir frapper à notre porte.
   Comme lui, Thomas a voulu garder un lien avec l’école et transmettre
   son savoir. C’est ainsi qu’il s’est lancé dans l’accompagnement d’un
   projet de…
   Lire la suite

   Archi & techno

Interview Céline Gilet – « Le Tech Lead n’est pas un super héros ! »

   Posté le 23/09/2019 par Céline Audibert
   [archi.png]

   Depuis plus de 4 ans chez OCTO, Céline, membre de la tribu CRAFT, est
   devenue une référence parmi nos Tech Lead. Découvrez sa vision de ce
   rôle à part. Pour toi, quel est le rôle du Tech Lead ?  Pour moi, c’est
   faire en sorte que l’équipe au sens large (Développeurs, Ops,
   Fonctionnels, Product Owner) arrive à délivrer régulièrement de la
   valeur. Concrètement, il s’agit de jongler et prioriser en permanence
   entre plusieurs casquettes : expertise, accompagnement, coaching et
   formation.
   Lire la suite

   Méthode innovation

Injonctions paradoxales : un MVP … mais pour tous !

   Posté le 20/09/2019 par Dominique Lequepeys, Sylvain Fagnent
   un MVP pour tous

   Un MVP ... mais pour tout le monde ! Et si vous vouliez l’entendre
   cette injonction ? Mise en scène, écoutez la. Les racines du paradoxe
   D'un côté, les managers sont sous la pression du timing : ils sont
   séduits par le concept de MVP, Minimum Viable Product, présenté comme
   un moyen d'accélérer la mise sur le marché. D'un autre, ils sont sous
   la pression du chiffre : ils ont du mal à accepter qu'on se prive d'une
   partie du marché potentiel. En outre, ils…
   Lire la suite
   1234>
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » Le chemin vers l’omnicanal Flux des commentaires Le
   demi-cercle (épisode 48 — Plaques tournantes) L’optimisation bayésienne
   par l’exemple : à quoi ça sert et comment ça marche ? alternate
   alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

Le chemin vers l’omnicanal

   Posté le 01/08/2018 par Julien Kirch
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Si votre système d’information n’est pas tombé dedans quand il était
   petit, faire de l’omnicanal est souvent un parcours semé d’embûches, et
   de promesses d’éditeurs.

   Cet article se propose de parler des sujets de fond liés à ce type de
   transformations.

   Il est aussi l’occasion d’aborder l’histoire des SI pour comprendre
   comment on est arrivé à la situation actuelle.

C’est quoi l’omnicanal ?

   Un canal est un point d’accès spécifique à un système. Un SI peut par
   exemple avoir un canal web client, un canal application mobile client,
   un backoffice de gestion, des accès pour les partenaires…

   Un SI omnicanal est un SI qui permet aux différentes personnes qui
   l’utilisent de passer de manière fluide d’un canal de distribution à un
   autre.

   L’exemple type est de commencer à faire une demande de prêt immobilier
   sur son smartphone, de la continuer sur son ordinateur portable une
   fois chez soi, puis éventuellement de terminer le dossier en se rendant
   en agence. Cela ne signifie pas utiliser le même outil dans les trois
   cas, mais bien d’avoir des outils spécifiques dont l’ergonomie est
   adaptée à chaque utilisation, et de pouvoir passer de l’un à l’autre
   « sans couture » (seamless), c’est-à-dire sans avoir à refaire une
   opération déjà faite comme une saisie de données.

   En plus des avantages pour les utilisateur·rice·s, nous allons voir que
   l’omnicanalité a également des avantages pour l’IT.

   À l’inverse, un SI multicanal fournit les mêmes fonctionnalités sur
   plusieurs canaux mais de manière silotée, et la bascule d’un canal à
   l’autre est donc visible de la part des personnes ou des systèmes qui
   l’utilisent.

   Dans ce cas une personne qui commence une demande de prêt immobilier
   chez elle et qui se rend en agence devra reprendre le dossier depuis le
   début car le backoffice de l’agence n’a pas accès à la demande faite
   sur le canal web client.

Pourquoi c’est compliqué ?

   Le chantier de transformation omnicanal comporte plusieurs axes, liés
   aux différentes contraintes des systèmes actuels.

   Pour comprendre la situation, le mieux est de revenir en arrière et de
   dérouler l’historique du SI.

   Nous allons prendre ici un exemple typique du domaine bancaire tel
   qu’on le retrouve chez de nombreux acteurs historiques.

Les années 80 : au commencement étaient le mainframe et le backoffice

   Les premières briques du SI se sont construites dans les années 80 sur
   mainframe, développées en COBOL ou équivalent. Ces systèmes historiques
   peuvent être des développements « maison », des progiciels, ou un
   mélange des deux.

   Les écrans de backoffice permettant d’y accéder sont conçus pour les
   employé·e·s de l’entreprise et leurs sont réservés.

   Les workflows de traitement et les fonctionnalités exposées sont
   directement calqués sur les écrans et chaque étape du process est
   stockée dans la base de données d’une manière structurée. Il ne s’agit
   pas d’un modèle MVC : les définitions des écrans sont imbriquées avec
   les traitements métiers.

   Les bureaux étant fermés la nuit et le week-end, les appels interactifs
   sont désactivés pendant ces périodes, ce qui permet d’exécuter des
   traitements de masse ou batch. Ces traitements bénéficient ainsi de
   l’intégralité de la puissance de calcul, et le fait d’être les seuls à
   s’exécuter leur permet de simplifier leur design car ils peuvent ainsi
   monopoliser des ressources comme des tables de bases de données sans se
   soucier du reste du monde.

   Cela permet aussi de simplifier les règles métier, par exemple les
   calculs comptables sont beaucoup plus simples lorsqu’aucune autre
   opération n’est effectuée pendant qu’ils s’exécutent.

Les années 2000 : l’arrivée du web, le bicanal

   Avec l’arrivée du web, il est temps d’ouvrir un site de banque en
   ligne.

   Cela signifie donner accès à des fonctionnalités du mainframe, mais
   d’une manière différente de celle du backoffice :
     * les écrans doivent être adaptés pour être utilisables par des
       non-employé·e·s, certains workflows comportent donc plus d’étapes ;
     * certaines options nécessitant la validation d’un·e employé·e
       empêcheront d’aller jusqu’au bout du traitement à partir du site,
       cela nécessitera des opérations de backoffice spécifiques.

   Le système mainframe historique est vital pour l’entreprise et la
   maîtrise qu’ell en a n’est pas toujours satisfaisante : ce patrimoine
   commence à dater et la connaissance s’est donc perdue, il comporte
   rarement des test automatisés et avec une documentation souvent
   lacunaire.

   La stratégie choisie est donc souvent de limiter au maximum l’ampleur
   des modifications sur cette partie du système pour limiter les risques.

   L’approche choisie alors consiste à exposer les workflows existant
   autant que possible – c’est-à-dire ceux du backoffice – principalement
   sous formes d’API synchrones, et à développer le site web au-dessus de
   ces API, alors même que les workflows ne sont pas les mêmes.

   Les contrats de ces APIs sont donc assez proches des écrans mainframe
   pour limiter l’effort à fournir. Il ne s’agit bien entendu pas d’API
   REST, mais généralement de messages MQ ou d’appels CTG.

   Lorsque les deux workflows ne correspondent pas, on aboutit à ce type
   de situation :

   Il est dans ce cas impossible de stocker les résultats des étapes 1A ou
   2A dans le mainframe. Ils seront donc stockés dans le backend du site
   web dans une base de données séparée. Cela signifie aussi qu’il faudra
   dupliquer les contrôles de saisie de ces étapes dans la partie web,
   pour éviter d’avoir à revenir en arrière dans les écrans du site web.

   Suivant les étapes, les données sont donc stockées soit dans le système
   cœur, soit de manière intermédiaire dans le sous-système du site web.

   En fonction des situations, les points de « rencontre » des workflows
   sont plus ou moins nombreux. Le cas extrême est celui où il existe un
   seul point de synchronisation : la dernière étape du workflow. Dans
   cette situation, le site web doit stocker toutes les données
   intermédiaires, et recoder tous les contrôles de saisie.

   Dans ce cas, les données dans la base du site web qui n’ont pas été
   déversées dans la base du mainframe ne sont ni visibles depuis le
   backoffice ni des autres systèmes qui exploitent cette base.

   Par exemple, si vous commencez à souscrire un prêt immobilier sur le
   site web sans terminer la procédure et que vous vous rendez dans votre
   agence bancaire, il faudra refaire tout ou partie des opérations.

   Par ailleurs, les opérations de backoffice spécifiques au site web
   ainsi que les besoins de support client nécessitent de développer des
   écrans spécifiques branchés sur le même backend.

   L’inaccessibilité du cœur système historique pendant la nuit pose aussi
   problème : il est inconcevable de faire de même pour un site web
   destiné au grand public.

   Il existe de nombreuses manières d’améliorer cette situation,
   l’approche souvent rencontrée consiste à :
    1. effectuer une copie de certaines données avant de couper le système
       mainframe, et s’en servir comme d’un cache en lecture seule
       accessible pendant la nuit, le cache sera désactivé lorsque les
       traitements de masse sont terminés ;
    2. ne pas exécuter les opérations qui nécessitent des écritures mais
       les enregistrer sous forme de demandes d’exécutions dans le backend
       du site web, et réaliser réellement les traitements le jour suivant
       à l’ouverture du mainframe.

   Cela rend le SI plus difficile à observer car les données sont
   distribuées entre les deux sous-systèmes.

   Bien entendu, même si la réutilisation de fonctionnalités existantes
   est privilégiée, certains besoins du site web nécessitent de développer
   des APIs spécifiques dans le cœur métier.

Aujourd’hui : le mobile et les partenaires

   L’arrivée du mobile pourrait signifier la mise en place d’une
   tricanalité. Mais les besoins mobiles sont souvent suffisamment proches
   des besoins web pour qu’ils s’appuient sur les mêmes systèmes. Dans
   quelques situations, il peut être nécessaire de stocker des données
   intermédiaires sur les terminaux, mais il ne s’agit pas d’un vrai
   troisième canal.

   Les écrans de backoffice ont souvent été remplacés par des technologies
   web. Mais pour limiter les impacts sur le mainframe, on conservera
   souvent les mêmes workflows, le nouveau backoffice n’aura donc pas à
   stocker de données.

   De même, le site web public a pu être refondu, mais toujours en
   subissant les contraintes de l’existant.

   En revanche, la banque a noué des partenariats. Ces partenaires peuvent
   par exemple vendre des prêts de la banque en marque blanche quand vous
   achetez un de leur produits.

   Les process nécessaires aux partenaires sont aussi différents du
   process historique que du process web, le système devient donc souvent
   tricanal. Prenons le cas où l’intégration se fait via un backend
   spécifique.

   Pour rester lisible, le schéma ne contient pas les backoffice dédiés
   aux canaux web et partenaires mais ils existent bel et bien, une
   personne du support peut donc avoir à jongler avec trois backoffices
   différents.

   Le canal partenaire ne pose pas le même problème que le canal web. En
   effet, un client qui commence à souscrire un prêt en marque blanche en
   achetant un bien voudra rarement conclure la transaction dans votre
   agence. En revanche, la multiplication des canaux rend la maintenance
   du système plus complexe quand on veut modifier un des workflows
   centraux qui sont exposés aux autres canaux ou changer une des règles
   de gestion dupliquée à plusieurs endroits.

   Certains besoins des partenaires se rapprochent de ceux du site web
   client, il arrive donc qu’une partie du code soit partagée entre les
   deux. Cela évite de développer plusieurs fois les mêmes choses mais
   rend le système encore plus difficile à observer.

En résumé : les problèmes du multicanal

   Le multicanal pose donc les problèmes suivants :
     * mauvaise expérience utilisateur·rice·s lors du passage d’un canal à
       l’autre ;
     * duplication de code entre les canaux ;
     * données partiellement dupliquées entre les canaux ;
     * limites dans la capacité à créer des parcours très différents du
       parcours historique ;
     * difficulté de mettre en œuvre des évolutions cross-canaux du fait
       de la duplication ;
     * système difficile à observer.

Que faut-il pour avoir un SI omnicanal ?

   Les problèmes causés par le multicanal et les limites des SI
   correspondants nous donnent les informations nécessaires pour dresser
   le plan d’un SI omnicanal.

   Avant de rentrer dans le détail, il faut préciser qu’un système
   omnicanal ne signifie pas un système unique de haut en bas pour tous
   les canaux mais un système cœur permettant de répondre aux besoins de
   l’omnicanalité sur lequel viendront se brancher les différents canaux.

   La différence avec un système multicanal est la capacité de passer d’un
   canal à l’autre, pas le fait d’avoir un système unique.

   Ainsi vous n’exposerez pas forcément les mêmes services ou les mêmes
   technologies pour votre application mobiles et pour vos partenaires.
   Vous aurez un système cœur sur lequel viendront se greffer votre canal
   backoffice, votre canal public, votre canal partenaire…

Des processus métier indépendants des canaux

   Les workflows étant différent d’un canal à l’autre, l’omnicanalité
   nécessite de concevoir des processus métier qui soient adaptables aux
   différents canaux.

   Cela signifie qu’il ne faut pas penser son processus en termes d’étapes
   qui ont la granularité d’un écran mais en termes de macro-étapes avec
   une taille plus importante, ce qui donnera à chaque canal les marges de
   manœuvres dont il a besoin.

   Par exemple, souscrire un crédit peut, en le simplifiant à l’extrême,
   se décomposer en trois macro-étapes :
     * renseigner des informations personnelles et faire des simulations
       de crédit jusqu’à obtenir une offre satisfaisante ;
     * valider une demande de crédit en saisissant des informations
       supplémentaires ;
     * traiter la demande dans le backoffice pour la valider ou la
       rejeter.

   Il s’agit d’un travail de conception métier. C’est souvent la partie la
   plus difficile du chantier car il s’agit d’un exercice dont on a peu
   l’habitude, et c’est donc une bonne première étape.

Un système de stockage

   Les données doivent être stockées dans un système indépendant des
   canaux.

   Comme les saisies d’informations peuvent se faire dans des ordres
   différents d’un canal à l’autre, on peut moins souvent s’appuyer sur
   des contraintes d’intégrité que dans un système monocanal.

   Par exemple un·e client·e pourra peut-être créer un compte sans fournir
   immédiatement son nom ou son adresse.

Des règles métier de validation

   Dans un système historique, les services métier étant adossés aux
   écrans, chacun comportait les règles métiers correspondantes permettant
   de valider les informations saisies dans le formulaire.

   Dans un système omnicanal, ce n’est plus possible car chaque canal peut
   concevoir son parcours.

   Cela signifie que les règles de validation seront sous deux formes :
    1. dans le système central, des règles de validation seront placées au
       niveau de chaque macro-étape ;
    2. les canaux doivent implémenter ces mêmes règles au niveau de chaque
       écran ou de chaque service exposé avec la granularité la plus fine
       possible pour être en mesure de remonter des erreurs au plus près
       de la saisie des données.

   Cela nécessite de bien documenter les règles.

Des services facilement utilisables et composables

   Ce sont les services synchrones et asynchrones sur lesquels seront
   construits les canaux.

   En effet, composer des services pour de l’omnicanal signifie de bien
   maîtriser les dépendances entre les différents services pour donner des
   libertés aux différents canaux.

   Ces services doivent aussi, autant que possible, être accessibles 24
   heures sur 24. Cela va nécessiter, du point de vue de l’extérieur, que
   les traitements ensemblistes « de nuit » ne rendent plus le système
   inaccessible. Cela peut demander de réutiliser le même type de
   comportements que ceux qui étaient utilisés par les canaux, comme le
   fait d’enregistrer des demandes d’exécutions à traiter plus tard. La
   différence est que le comportement sera cohérent entre les différents
   canaux car réalisé dans la partie commune.

Les canaux

   C’est la partie spécifique à chaque canal qui définit le workflow de ce
   canal et l’expose de la manière appropriée par des écrans ou des
   services.

   L’objectif est que cette partie du SI ne stocke pas d’information. En
   effet, comme nous l’avons vu plus haut, toute information stockée au
   niveau d’un canal va créer un silotage. Ils ne font que s’appuyer sur
   les services de la couche cœur.

   L’omnicanalité rend la conception des canaux plus difficiles car ils
   doivent prendre en compte le fait qu’un processus peut avoir été
   démarré dans un autre canal ayant un workflow différent.

   Par exemple, certains des champs de saisie auront peut-être déjà être
   remplis et pas d’autres.

   Il faut qu’il puisse déterminer comment effectuer la reprise du
   traitement dans de bonnes conditions.

   Cela demande une conception rigoureuse ainsi qu’une bonne couverture de
   tests.

Faire vivre le système

   La dernière pierre de l’omnicanal est la capacité à le faire vivre.

   En effet, les canaux sont fortement couplés au système cœur, ils
   devront donc être modifiés de manière coordonnée.

   Ce couplage est un effet direct de l’omnicanalité : c’est elle qui
   permet de passer d’un canal à l’autre. Le modèle de canaux découplés
   est celui du multicanal.

   Votre organisation doit donc être adaptée à cette contrainte.

Comment y aller ?

   Maintenant que nous savons en quoi devrait consister un système
   omnicanal, reste à étudier les trajectoires pour l’atteindre.

   Nous allons commencer par un point sur la situation de départ puis
   donner quatre exemples de stratégie possibles. Il existe de multiples
   approches, celles qui sont mentionnées ici ont été choisies car elles
   mettent en lumières les contraintes qui s’appliquent.

Situation de départ

   Le système multicanal comporte deux éléments qui ont de la valeur et
   sur lesquels il faut s’appuyer en le faisant évoluer vers l’omnicanal,
   et deux limites qu’il faudra supprimer :

   À conserver :
     * les règles de traitement métier ;
     * les règles de validation de données.

   Les deux représentent de la valeur même si elles sont adhérentes au
   étapes du workflow historique (par exemple les différents écrans du
   process de souscription originel).

   À supprimer :
     * le workflow unique formant l’assise du système historique
     * les règles d’intégrité des données alignées avec le process
       historique

Stratégie 1 : commencer par acheter un BPM

   C’est la solution que préconisent certains éditeurs.

   Les BPM sont des outils permettant de définir des workflow métiers sous
   forme low-code, c’est-à-dire via de la configuration et/ou des
   designers graphiques. Ils permettent également de stocker l’état
   courant des différents workflows.

   C’est une solution tentante car elle fournit un socle prêt à l’emploi
   pour une partie des besoins.

   Deux points d’attention pour cette approche :
     * comme avec tout progiciel, attention à ne pas oublier les bonnes
       pratiques de développement comme les tests automatisés : votre BPM
       embarquera du code, et qui dit code dit tests ;
     * ne pensez pas qu’avoir choisi un BPM signifie que vous avez gagné,
       en effet nous avons vu que la partie la plus difficile du chantier
       est la conception des services sur lesquels va s’appuyer le BPM.

   Il s’agit d’une utilisation très spécifique des outils de BPM, loin de
   la gestion des processus métiers qui est leur utilisation normale.

Stratégie 2 : repartir sur un nouveau système

   C’est la solution la plus risquée, mais qui est parfois la moins
   mauvaise. Par exemple quand vous avez perdu la maîtrise de votre
   système historique, ou qu’il s’agit d’un progiciel qui n’est pas
   compatible avec l’omnicanal.

   La solution n’est pas forcément de partir de zéro : il est possible de
   partir sur un progiciel plus récent, ou de racheter une entreprise
   disposant d’une solution déjà fonctionnelle.

Stratégie 3 : rendre le cœur métier historique omnicanal

   Il s’agit d’attaquer le problème par le bas, c’est-à-dire par le cœur
   métier.

   Cela peut être à l’occasion de l’ajout d’un nouveau canal, en profitant
   d’avoir des nouveaux besoins factuels, et un budget.

   Il va s’agir de transformer le cœur, puis de faire maigrir les canaux
   existants en redescendant ce qui ne devrait pas s’y trouver, comme le
   stockage de données.

   La situation de départ


   En cours de migration : les canaux diminuent et le cœur s’enrichit


   Cible : les canaux n’ont plus de base de données

   C’est probablement la meilleure solution si vous avez la maîtrise de
   votre existant et que vous souhaitez capitaliser dessus.

   Deux points d’attention :
     * faire évoluer de manière significative un outil demande un niveau
       de maîtrise plus important que le fait de le maintenir, la facilité
       à corriger des erreurs sur le cœur n’est pas un bon indicateur de
       votre capacité à le transformer ;
     * ne pas introduire de régressions, par exemple en supprimant des
       comportements non documentés mais sur lesquels le code s’appuie.

Stratégie 4 : ajouter une couche d’omnicanal au-dessus du cœur

   Il s’agit de la voie intermédiaire : on s’appuie sur l’existant le
   temps de bâtir un remplacement.

   Il s’agit de bâtir une surcouche omnicanale au-dessus du cœur. Plutôt
   que de partir de zéro, il est possible de partir d’un des canaux
   existants en le séparant entre une partie souche qui servira de base à
   la partie omnicanal et la partie exposition qui deviendra la nouvelle
   couche canal.

   En enrichissant peu à peu de nouveau types de données en les remontant
   depuis le cœur historique et des fonctionnalités associées. Cette
   couche devra exposer les services réutilisables qui serviront de base
   aux différents canaux.

   Pendant la construction, vous continuerez de subir les limitations du
   cœur existant, mais commencerez à bénéficier de certains avantages de
   l’omnicanalité, comme la transition plus facile d’un canal à l’autre.

   L’étape suivante consistera à dégonfler le système historique pour
   s’appuyer de plus en plus sur la nouvelle couche.

   Cela va probablement demander des évolutions du système cœur. Cependant
   elles ne demanderont pas de transformations profondes, au contraire de
   la stratégie précédente.

   En cible on pourra décomissionner totalement le système historique, ou
   conserver certains éléments comme les parties réglementaires pour
   lesquels la migration ne se justifie pas et qui n’imposent pas de
   contraintes sur le nouveau système.

   Une des difficultés de cette stratégie est de bien choisir l’ordre dans
   lequel remonter les fonctionnalités pour bénéficier au plus vite des
   premiers avantages tout en limitant les risques.

   La situation de départ

   En cours de migration, la zone du milieu prend de l’importance

   Cible : le cœur historique n’est plus le centre du système

Pour terminer

   L’omnicanalisation d’un SI est un chantier risqué et de longue haleine.
   Mal conçu ou mal piloté, il peut être un enfer de plusieurs années qui
   aboutira à ajouter de nouvelles briques à votre système, sans atteindre
   aucun des buts fixés.

   Il est autant lié à la DSI qu’au métier : il demande du travail à tous
   les deux, mais apportera aussi des avantages à chacun. Si l’un des deux
   acteurs veut se lancer sans la pleine coopération de l’autre, c’est
   l’échec presque assuré.

   Même si ce changement peut permettre de réduire la dépendance aux
   systèmes historiques, y arriver va demander de comprendre comment ces
   systèmes fonctionnent, et de les modifier. Moins bien vous maîtrisez
   votre mainframe, plus il sera difficile de vous en passer.

   Si un tel projet vous semble long et coûteux aujourd’hui, gardez à
   l’esprit que plus le temps passe et plus la situation va empirer.

   Bonne chance à vous.
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans Archi & techno, Stratégie SI et taggué
   Architecture SI, omnicanal.

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Un commentaire sur “Le chemin vers l’omnicanal”

     Anas
   02/08/2018 à 16:20
   Une stratégie qui n'a pas été évoquée et qui mérite de l'être, il
   s'agit de la solution CEP couplée à une solution de suivi de parcours.
   La solution BPM n'est pas très adaptée à des parcours impliquant des
   millions d'utilisateurs car il faut pouvoirs stocker et gérer autant
   d'instances de processus que de parcours utilisateurs en cours. Pour
   peu que le parcours dure quelques jours voir quelques semaines (ce qui
   est le cas dans le domaine bancaire), on atteint très vite des volumes
   difficilement supportables par les solutions BPM du marché.

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha

   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » Le demi-cercle (épisode 49 — Cocktail) Flux des
   commentaires L’optimisation bayésienne par l’exemple : à quoi ça sert
   et comment ça marche ? GraphQL: Et pour quoi faire ? alternate
   alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

Le demi-cercle (épisode 49 — Cocktail)

   Posté le 03/08/2018 par Christophe Thibaut
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   The world is not interested in the storms you encountered, but did you
   bring in the ship?
   William McFee

   Sept heure moins dix. Tu entres dans la Grande Salle de la Direction
   Générale. Toutes les lumières sont allumées bien que le jour soit
   encore clair. On a plié toutes les tables sauf une, et repoussé les
   chaises dans un coin.

   Pop !

   Victor sert le champagne dans des flûtes. Tu te demandes si elles ont
   été louées pour l’occasion, ou si décidément cette maison regorge de
   trésors cachés. Il y a du monde, au moins 40 personnes. Audrey et
   Jérémie sont en train de discuter avec un homme en costume trois pièces
   et une jeune femme d’apparence assez chic malgré une chevelure hirsute.
   Certainement des clients de chez Juniper. Tu t’approches. Audrey dit :
   – En tout cas, c’est une bonne chose que vous ayez pu rencontrer
   Victor. J’étais loin de maîtriser mon sujet sur Parady !
   La jeune femme dit :
   – Vous vous en êtes très bien sortis. Une présentation très
   professionnelle, vraiment.
   Jérémie sourit d’un sourire que tu ne lui connais pas, et répond :
   – Merci. Beaucoup.
   L’homme en costume dit :
   – Et surtout, ça nous a permis de mieux faire connaissance avec votre
   produit. Ce qu’on ne vous a pas dit, c’est que ces features que vous
   nous avez présentées, on les cherche depuis deux ans sur le marché !
   Une chance que Victor se soit planté en scooter.
   Mais non, qu’est-ce que tu racontes.
   Il aurait pris tout l’espace et le client n’aurait pas vu la démo
   d’Audrey et Jérémie, et il ne nous aurait pas acheté la gamme complète.
   La médisance…

   Farid et Hugo sont arrivés. Ils serrent des mains. Prennent une flûte.

   Tintement de verre. Les conversations s’estompent.

   Le Président Directeur Général, Gérard Beaufret, lève son verre un
   instant, le pose sur la table derrière lui, et déclare :
   – Je voudrais remercier d’abord nos clients, anciens et nouveaux (il
   adresse un sourire au couple venu de Juniper), merci, pour la confiance
   que vous nous faites, et qui nous honore. Nous cherchons constamment à
   améliorer la qualité de nos produits, et vos appréciations — et bien
   entendu vos critiques — nous sont très très utiles.

   Bien entendu.
   Stop.

   Le PDG reprend :
   – Je voudrais remercier l’équipe XXL…
   Il fait un geste pour vous inviter Audrey, Jérémie, Hugo, Farid et toi
   à entrer dans le grand cercle. Vous avancez. Tu souris.

   Tu souris trop.
   Mais non.

   – … sans qui cette épopée ne serait pas possible. Chapeau ! Et sans
   oublier bien sûr, notre chère Maria, et Victor qui reprend le flambeau
   de plus belle.

   Tout le monde applaudit, y compris vous. Tu applaudis tes coéquipiers.
   Maria applaudit. Victor applaudit. Jean-Bernard applaudit. Lazare lève
   sa flûte de champagne et adresse un clin d’œil à Jérémie.

   Pas facile d’applaudir avec une flûte à la main.
   Même vide.

   Une bonne demi-heure se passe. Brouhaha de discussions animées, sur
   l’avenir du logiciel, la digitalisation, la vitesse, la réactivité.
   Jérémie, Farid et toi, vous parlez d’architecture. Victor, accompagné
   d’un collègue de la Direction des Ventes, s’immisce dans le groupe et
   le présente :
   – Vous connaissez Daniel Derby ? Product Manager de la gamme Parady.
   Jérémie dit : salut Daniel.

   Bon sang Jérémie connaît tout le monde dans cette boîte.

   Vous vous saluez. La conversation fait comme une pause, puis reprend :

   Daniel : Alors Jérémie, dis nous, c’est quoi votre secret de
   fabrication, chez XXL ?
   Jérémie : Ah ah. Le secret… Mais il n’y a pas de secret.
   Daniel : Sans blague. Regardez le chemin parcouru en 10 mois. Vous avez
   forcément un truc. Comment faites-vous pour aller si vite ?
   Farid : Oui, tiens c’est vrai, c’est quoi notre secret ?
   Jérémie : Pffff. Non, je ne vois pas.
   Victor (s’esclaffant) : Il faut continuer à le faire boire si tu veux
   découvrir le truc. Je vais chercher du champagne.
   Daniel : Si tu y réfléchis, là comme ça ? Qu’est-ce que vous avez de
   différent ?
   Toi : On a un bureau qui donne sur le sud ? Non, je plaisante.

   Victor revient avec du champagne et s’avance pour remplir vos flûtes.
   Jérémie, Farid et toi, déclinez poliment.

   Jérémie (tend son verre) : Oh, remarque après tout. Merci.

   Jérémie te regarde une seconde, déguste son champagne et reprend :
   Jérémie : Notre secret, c’est la qualité des conversations.
   Farid (claque des doigts) : Voilà. Exactement !
   Daniel : La qualité de vos conversations ? C’est une blague ?
   Jérémie : Mais non, pourquoi ? Qu’est-ce qui te fait rire là-dedans ?
   Victor : Remarque qu’il n’a pas tort. Au début ça m’a surpris, mais en
   fait il a un peu raison.
   Daniel : Mais, vous n’êtes pas là pour avoir de jolies conversations de
   salon ! Vous êtes là pour faire votre travail.
   Jérémie : Bien sûr. Moi-même, je n’aime pas les conversations de salon
   comme tu dis. Non, je te parle des conversations au travail. Celles qui
   permettent d’obtenir des informations et de prendre nos décisions.
   Daniel : Ha ! Vous prenez des décisions ! Et combien de décisions vous
   prenez de cette manière ?
   Jérémie : Je dirais plusieurs dizaines par jour. C’est principalement
   ce en quoi consiste notre travail d’ailleurs.
   Farid : Une longue série de décisions…
   Toi : Plus ou moins funestes…
   Farid (souriant) : Tu vois le verre à moitié vide…
   Daniel (vide sa flûte, croise les bras) : Tout de même. Je suis curieux
   de savoir quel type de conversations vous tenez, et à propos de quoi ?
   Jérémie : Ça dépend pas mal de ce que l’on est en train de faire, note.
   Daniel : Ah oui ?
   Jérémie : Mais si on voulait les catégoriser par contenu, je pense
   qu’on trouverait cinq catégories distinctes :
   (Jérémie compte sur ses doigts) Les conversations avec le code. Les
   conversations à propos du code. Les conversations à propos du problème
   que nous voulons résoudre grâce au code. Les conversations à propos de
   notre façon de résoudre le problème
   Toi : Ça fait quatre.
   Jérémie (souriant) : Je sais encore compter. Et puis les conversations
   à propos de toutes ces conversations.
   Daniel : Attends, j’ai perdu le fil. Qu’est-ce que tu appelles une
   conversation avec le code ?
   Jérémie : Tu lis un morceau de code, tu ne le comprends pas bien;
   disons que sa forme est difficile à comprendre. Tu écris un test sur ce
   code, qui te révèle une nouvelle information, ou bien qui confirme une
   hypothèse. C’est comme si tu posais une question. Quand tu as
   suffisamment de tests, tu peux changer la forme du code, ce qui le rend
   plus facile à comprendre. Et tu relances tes tests. C’est ce que
   j’appelle une sorte de conversation. Si on veut.
   Daniel : Ça me rappelle mes études. C’est ce qu’on faisait avec
   Smalltalk. Sauf qu’on ne faisait pas de tests comme vous, on utilisait
   un débogueur intégré.
   Jérémie : Je ne connais pas Smalltalk, mais je dirais qu’on peut faire
   ça avec n’importe quel langage, du moment qu’on a un outil de test et
   un débogueur.
   Daniel : Bon. Admettons. Et les autres conversations ?
   Jérémie : Les conversations à propos du code, sont celles que nous
   avons très souvent entre développeurs. Est-ce que le code fait ce qu’il
   est supposé faire ? Est-ce que le code est facile à comprendre ?
   Qu’est-ce qu’il faut changer dans ce code ? Ce genre de questions.
   Daniel : Je vois. Et ensuite ?
   Jérémie : Les conversations à propos du problème, sont celles que nous
   avons avec Maria, Charlène, Victor et d’autres personnes, et ces
   conversations tournent autour des utilisateurs, des clients et de ce
   qu’ils essayent d’obtenir au moyen d’XXL, et comment ils l’obtiennent.
   Daniel : OK. C’est la partie fonctionnelle.
   Jérémie : Si tu veux.
   Daniel : Mais ça, ça ne devrait pas faire l’objet de conversation
   justement : ça devrait être dans la spécification.
   Jérémie : Ah bon ?
   Daniel : Bien sûr. Pour écrire un programme qui fait correctement ce
   qui est attendu fonctionnellement, il faut des spécifications.
   Jérémie : OK. Je ne dis pas non. Dans ce cas, cette catégorie de
   conversation inclura probablement les conversations à propos de la
   spécification.
   Daniel : Eh bien non, pas du tout. La spécification est justement là
   pour éviter d’avoir à revenir sur le sujet.
   Jérémie : Pour que ce que tu dis soit exact, à savoir, qu’on aie pas
   besoin de revenir sur le sujet, il faudrait que la spécification soit
   complète.
   Victor : Ah ça…
   Jérémie : Note que si la spécification était complète, on pourrait
   probablement la transformer automatiquement en logiciel. On n’aurait
   plus besoin de la traduire en code.
   Daniel : Ha ! C’est ça qui serait pratique !
   Victor : Mais alors les développeurs n’auraient plus de travail… Et
   alors tu ferais quoi, comme métier, Jérémie ?
   Jérémie : Probablement un métier qui tourne autour de la création de
   spécifications.
   Daniel : Bon. D’accord. Ensuite, tu as parlé des conversations à propos
   de notre façon de résoudre le problème…
   Jérémie : Oui, c’est tout ce qui concerne le process, l’organisation.
   La méthodologie si tu préfères.
   Victor : Et la dernière catégorie ?
   Jérémie : Les conversations à propos de toutes ces conversations.
   Daniel : Oui, en quoi ça consiste ?
   Jérémie : Eh bien, par exemple, à essayer de comprendre pourquoi une
   conversation qui aurait dû être plus efficace par exemple, ne l’a pas
   été, et ce qu’on peut faire pour qu’elle le devienne.
   Daniel : Et donc, comme ça, vous savez, à tout moment, quel type de
   conversation vous êtes en train d’avoir ? C’est hilarant.
   Jérémie : Mais non, pas du tout.
   Daniel : Alors pourquoi tu me parles de toutes ces catégories Jérémie ?
   Jérémie : Tu m’as demandé quels types de conversation on tient. J’ai
   essayé de répondre à cette question.
   Daniel : Ha ! Tu es trop sérieux, Jérémie. Tiens bois encore un peu de
   champagne.
   Toi : Il se fait tard. Je rentre. A demain !

   (à suivre)
   Episodes Précédents :
   1 — Si le code pouvait parler
   2 — Voir / Avancer
   3 — Communication Breakdown
   4 — Driver / Navigator
   5 — Brown Bag Lunch
   6 — Conseils à emporter
   7 — Crise / Opportunité
   8 — Le Cinquième Étage
   9 — Que faire ?
   10 — Soit… Soit…
   11 — Boîtes et Flêches
   12 — Le prochain Copil
   13 — La Faille
   14 — Poussière
   15 — L’hypothèse et la Règle
   16 – Déplacements
   17 — Jouer et ranger
   18 — Arrangements
   19 — Mise au point
   20 — Expérimentation
   21 — Échantillons
   22 — Non-conclusions
   23 — Non-décisions
   24 — Épisode neigeux
   25 — Fusions et confusions
   26 — Débarquement
   27 — Tempête
   28 — Embardée
   29 — Aménagement
   30 — Interruptions
   31 — Normalisation
   32 — Outsiders
   33 — Fabrication
   34 — Observation
   35 — Perturbations
   36 — Conclusions
   37 — Nouvelle Donne
   38 — Transaction
   39 — Mutation
   40 — Exclusion Mutuelle
   41 — Préemption
   42 — Démonstration
   43 — Conversation
   44 — Exception
   45 — Explications
   46 — Télescopage
   47 — Négociations
   48 — Plaques tournantes
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans Software Craftsmanship.

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

   Sécurité

Comment conserver les mots de passe de ses utilisateurs en 2019 ?

   Posté le 02/10/2019 par Fabien Leite
   Mot de passe

   Lorsque vous concevez une application, vous vous posez forcément la
   question de l’authentification et du contrôle d’accès. Pour ça,
   plusieurs méthodes sont disponibles et la première qui vient
   généralement à l’esprit est l’utilisation d’un couple identifiant / mot
   de passe. Dans la mesure du possible, on préfèrera utiliser une
   solution dédiée à l’authentification et au contrôle d’accès : en bref,
   utiliser une solution d’IAM pour gérer ces aspects à votre place. C’est
   généralement plus simple à maintenir et c’est surtout souvent meilleur
   pour l’expérience utilisateur. …
   Lire la suite

   Méthode

Amélioration continue : Comment rester dynamique à mesure que l’équipe
s’agrandit ?

   Posté le 01/10/2019 par Etienne Girot

   Différentes études1 soutiennent qu’une équipe performante est une
   équipe qui est capable de remettre fréquemment en question ses modes de
   fonctionnement afin d’apprendre et s’améliorer en continu. Pour y
   arriver, elle : favorise l'émergence de nouvelles idées a moyen de
   valider ou d’invalider efficacement la pertinence de ces nouvelles
   idées est en mesure d’aligner ses membres derrière les idées retenues
   comme étant pertinentes Or, plus une équipe grandit (aussi bien en
   nombre de membres qu’en temps passé à travailler ensemble) plus elle
   est sujette à…
   Lire la suite

   Méthode innovation

Culture Innov’ : Quel ROI attendu ?

   Posté le 01/10/2019 par Sylvain Fagnent, Matthieu VETTER
   [school.png]

   Après des années et des millions investis sous la menace de la
   disruption, les Directions reviennent à une logique plus ROIste. Ce
   mouvement est sain pour optimiser les ressources rares et donner du
   sens au travail des innovateurs. Le risque est cependant de tuer dans
   l’oeuf des pépites potentielles en pilotant l’innovation comme un
   business opérationnel. En fonction des objectifs, les retours sur
   investissement (ROI) d’une démarche d’innovation seront différents. Et
   dans un monde de plus en plus focalisé sur la rentabilité à court
   terme,…
   Lire la suite

   Data Science

Ouvrir la boîte noire et comprendre les décisions des algorithmes

   Posté le 30/09/2019 par Annabelle Blangero
   [school.png]

   L’usage des algorithmes de traitement de données – de la simple requête
   SQL aux puissants algorithmes de recommandation et de personnalisation
   des géants de la Tech – s’est popularisé ces dernières années,
   notamment pour des utilisateurs traditionnellement hors du domaine IT.
   Cet usage se retrouve dans tous les secteurs (industrie, éducation,
   santé, sécurité, etc.) et tend à déléguer de plus en plus de décisions
   à des systèmes automatisés. Cette appropriation par le plus grand
   nombre rend les naufrages encore plus probables, et l’exemple de
   Cambridge…
   Lire la suite

   Archi & techno

Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos valeurs et le
garant de la vision technique d’OCTO.”

   Posté le 27/09/2019 par Joy Boswell
   [archi.png]

   Chez OCTO depuis plus de 10 ans , Meriem fait partie des personnes
   fondatrices de l’entreprise. Ancienne leadeuse de la tribu Nouvelles
   Architectures de Données, elle est désormais CTO et participe à la
   définition de la vision stratégique et technique d’OCTO. Qui de mieux
   pour nous parler du “tech leading à la OCTO” ?
   Lire la suite

   Data Science

Mise en application de DVC sur un projet de Machine Learning

   Posté le 27/09/2019 par Nicolas TOUSSAINT, Jérémy Bouhi
   [school.png]

   Introduction DVC (Data Version Control) est un package Python qui
   permet de gérer plus facilement ses projets de Data science. Cet outil
   est une extension de Git pour le Machine Learning, comme l’énonce son
   principal contributeur Dmitry Petrov dans cette présentation. DVC est à
   la fois comparable et complémentaire à Git. Il va s’occuper de
   synchroniser vos données et votre code. Il est donc particulièrement
   intéressant dans le cadre d’un projet de Machine Learning où le modèle
   et les données évoluent au fil du développement.…
   Lire la suite

   Archi & techno

BD – Le Déploiement Continu (CD)

   Posté le 26/09/2019 par Aryana Peze
   [archi.png]

   Hello ! Lors de la BD précédente, nous avons abordé le sujet de la CI
   (Intégration Continue). Et impossible de parler de CI sans parler de CD
   (Déploiement Continu)! En thoérie, la CD implique un déploiement
   automatique et quasi-systématique de chaque modification du code sur
   l'environnement de production. Les mises en production sont régulières
   et ne sont plus une source de stresse, et l'environnement de production
   est ainsi toujours à jour. En pratique, c'est un objectif très
   compliqué à atteindre, et pas toujours adapté. (Petite parenthèse…
   Lire la suite

   Agile

D’étudiant à mentor : rencontre avec notre Octo Thomas Le Flohic

   Posté le 24/09/2019 par Céline Audibert
   [agile.png]

   Thomas a fait un véritable parcours “à la OCTO” : après son école
   d’ingé, il rentre en stage au sein de notre tribu VIBE (Virtual
   Immersion and Bot Experience) et rejoint définitivement l’entreprise en
   intégrant le programme Skool. Un de ses profs à l’école était un Octo,
   Fabien. C’est ce qui lui a donné envie de venir frapper à notre porte.
   Comme lui, Thomas a voulu garder un lien avec l’école et transmettre
   son savoir. C’est ainsi qu’il s’est lancé dans l’accompagnement d’un
   projet de…
   Lire la suite

   Archi & techno

Interview Céline Gilet – « Le Tech Lead n’est pas un super héros ! »

   Posté le 23/09/2019 par Céline Audibert
   [archi.png]

   Depuis plus de 4 ans chez OCTO, Céline, membre de la tribu CRAFT, est
   devenue une référence parmi nos Tech Lead. Découvrez sa vision de ce
   rôle à part. Pour toi, quel est le rôle du Tech Lead ?  Pour moi, c’est
   faire en sorte que l’équipe au sens large (Développeurs, Ops,
   Fonctionnels, Product Owner) arrive à délivrer régulièrement de la
   valeur. Concrètement, il s’agit de jongler et prioriser en permanence
   entre plusieurs casquettes : expertise, accompagnement, coaching et
   formation.
   Lire la suite

   Méthode innovation

Injonctions paradoxales : un MVP … mais pour tous !

   Posté le 20/09/2019 par Dominique Lequepeys, Sylvain Fagnent
   un MVP pour tous

   Un MVP ... mais pour tout le monde ! Et si vous vouliez l’entendre
   cette injonction ? Mise en scène, écoutez la. Les racines du paradoxe
   D'un côté, les managers sont sous la pression du timing : ils sont
   séduits par le concept de MVP, Minimum Viable Product, présenté comme
   un moyen d'accélérer la mise sur le marché. D'un autre, ils sont sous
   la pression du chiffre : ils ont du mal à accepter qu'on se prive d'une
   partie du marché potentiel. En outre, ils…
   Lire la suite
   1234>
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » L’optimisation bayésienne par l’exemple : à quoi ça sert
   et comment ça marche ? Flux des commentaires Le chemin vers l’omnicanal
   Le demi-cercle (épisode 49 — Cocktail) alternate alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

L’optimisation bayésienne par l’exemple : à quoi ça sert et comment ça marche
?

   Posté le 02/08/2018 par Louis Boutin, Paul De Nonancourt
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   “Si j’ai une valeur y qui est fonction de x, comment faire pour
   déterminer la valeur de x minimisant ou maximisant la valeur de y ?”
   tel est le problème de base du domaine de l’optimisation, qui se
   décline à de très nombreux cas d’usage allant de “comment fixer le prix
   pour maximiser un profit” à “quelle stratégie mon robot doit-il adopter
   pour rester en équilibre”.

   Nous vous proposons dans cet article une introduction aux stratégies
   d’optimisation bayésienne, un sous-domaine regroupant des techniques
   très puissantes pour converger efficacement vers des valeurs optimales
   lorsqu’on fait face à une situation où le nombre d’observations est
   limité par des contraintes de temps ou de matériel.

   Vous souhaitez voir un cas d’usage de ces algorithmes ? Comprendre
   comment ces derniers fonctionnent et comment les implémenter ? Alors
   vous êtes au bon endroit. Cet article prend pour exemple la
   problématique de la configuration automatique des paramètres d’une base
   de données pour illustrer l’optimisation bayésienne. Nous présenterons
   un exemple d’implémentation en Python ainsi que quelques autres cas
   d’usage pour cette approche.

1 – L’optimisation bayésienne au service de la performance d’une base de
données

1.1 – Un exemple concret

   Chez OCTO Technology, nous avons récemment décidé d’explorer des
   questions de configuration automatique du fonctionnement de bases de
   données, un domaine particulièrement large mais avec plusieurs
   problématiques que l’on peut distinguer. L’une d’elles en particulier
   va nous intéresser ici car elle constitue un bon exemple de situation
   où l’optimisation bayésienne se révèle pertinente.

   Imaginons une base de données, pas nécessairement complexe, avec un
   ensemble de paramètres à régler : cache, mémoires partagées, nombre de
   connexions simultanées maximum, etc. Étant donné une charge de travail,
   tel qu’un ensemble de requêtes exécutées sur cette base de données, on
   souhaite déterminer automatiquement les valeurs de paramètres qui nous
   permettront d’obtenir les meilleurs résultats (par exemple, nous
   voudrions compléter l’exécution de notre charge le plus rapidement
   possible). Il existe des règles métier fournissant des pistes pour
   fixer ces paramètres : le site PgTune propose par exemple des
   configurations pour Postgres en fonction du hardware (RAM, CPU). Mais
   ces règles ne sont pas forcément adaptées à notre charge de travail,
   sont rarement optimales et ne s’appuient pas nécessairement sur des
   théories. Pour résumer, on souhaiterait rendre le choix d’une bonne
   configuration le plus automatisé et le plus efficace possible mais il
   est en réalité très difficile de déterminer des règles précises
   régissant ces paramètres.

   Heureusement pour nous, il s’agit finalement d’un problème classique
   d’optimisation pour lequel de nombreuses solutions existent : nous
   voulons trouver rapidement une configuration (c’est-à-dire un ensemble
   de paramètres) maximisant ou minimisant une métrique de performance.
   Pour cela, nous devons tout de même prendre en compte un certain nombre
   de contraintes :
     * Tester une configuration prend du temps car nous devons tester une
       charge dans son intégralité sur notre base de données, nous ne
       pouvons donc pas tester un nombre illimité de configurations pour
       déterminer la meilleure.
     * Le comportement de la performance de la base de données en fonction
       de la configuration est une boîte noire, nous ne pouvons que donner
       une configuration en entrée et observer une performance en sortie.

   on optimise la configuration d'une une base de données en fonction
   d'une charge et une métrique de performance

   Le problème de départ : comment trouver rapidement la configuration
   optimisant la performance, c’est-à-dire ici minimisant le temps
   d’exécution ?

2.2 – Les avantages de l’optimisation bayésienne

   Compte tenu de nos contraintes, il est nécessaire de trouver une
   stratégie d’optimisation pas à pas efficace : nous souhaitons, en un
   minimum d’observations, déterminer la meilleure configuration. En
   consultant la littérature académique, nous avons ainsi identifié deux
   articles explorant également le problème de la configuration
   automatique d’une base de données : iTuned (2009) et Ottertune (2017).
   Les auteurs de ces deux publications proposent des solutions similaires
   basées sur l’optimisation bayésienne afin de rechercher une
   configuration optimale.

   Cette approche séquentielle se trouve être particulièrement adaptée à
   notre problème de par son principe. L’objectif est d’utiliser un petit
   nombre d’observations pour estimer un comportement plus global. En
   exploitant efficacement la connaissance accumulée sur notre fonction
   boîte noire, on espère minimiser le nombre d’observations et converger
   rapidement vers la configuration optimale.

   Étudions maintenant plus en détail le fonctionnement d’un algorithme
   d’optimisation bayésienne avant de passer à une implémentation concrète
   pour répondre à notre problème.

2 – Sous le capot de l’optimisation bayésienne

2.1 – Extraire de l’information à partir d’un nombre limité d’observations

   L’optimisation bayésienne est une approche probabiliste basée sur
   l’inférence bayésienne. En somme, cela veut dire qu’on va chercher à
   exploiter ce qu’on connaît déjà, donc l’ensemble des événements
   précédemment observés, pour inférer la probabilité des événements que
   nous n’avons pas encore observés. Dans le cadre de l’optimisation
   bayésienne, nous partons d’un ensemble d’observations dont nous
   connaissons le résultat et nous déterminons pour chaque valeur ???? en
   dehors de cet ensemble la distribution de probabilité de l’évaluation
   de ???? en ce point ????.


   Dans cet exemple, on cherche à minimiser une fonction qu’on ne connaît
   pas. Comment utiliser ces premières observations pour inférer les
   valeurs lorsque x varie entre -2 et 2 ?


   Mais comment calculer cette distribution de probabilité ? Parmi toutes
   les méthodes existantes, nous retiendrons ici l’une des méthodes
   classiques qui consiste à utiliser les processus gaussiens qui
   généralisent le concept de loi normale aux fonctions. Nous ne
   développerons pas cette méthode un peu complexe. L’important est de
   comprendre que cette approche nous permet de générer pour chaque point
   une distribution de probabilité caractérisée par une moyenne µ (la
   valeur la plus probable) et un écart-type σ (la mesure de la dispersion
   probable de la valeur autour de la moyenne).

   Pour le lecteur souhaitant aller plus loin, cet excellent article de
   blog développe davantage les processus gaussiens sans aller trop loin
   dans les détails mathématiques. La publication originale est ensuite la
   source la plus exhaustive sur le sujet.

   Les processus gaussiens permettent d’inférer la distribution de
   probabilité pour chaque point avec la valeur moyenne estimée µ en
   pointillé, et l’écart-type σ représenté par la zone vert pâle.

   Il est à noter qu’on ne pourra pas représenter cette distribution pour
   tous les points. Nous ne considérons cette distribution que sur un
   ensemble fini de points : c’est notre espace de recherche. Ainsi nous
   aurons, pour chaque point de cet espace de recherche, une valeur
   centrale, la moyenne μ, et une certaine dispersion autour de cette
   moyenne, l’écart-type σ. Ce dernier sera d’autant plus faible que l’on
   sera proche d’un point déjà observé.

   Nous sommes à présent capable d’inférer le comportement de notre
   fonction à partir de nos évaluations. Mais comment choisir quel point
   évaluer : quel point a les meilleures chances d’être un minimum ?

2.2 – Déterminer les points à plus fort potentiel avec la fonction
d’acquisition

   Rappelons-nous notre objectif : minimiser ou maximiser la fonction
   observée, c’est-à-dire trouver le point pour lequel l’évaluation est
   minimisée ou maximisée. Le choix du point utilisé pour l’évaluation
   suivante est donc soumis à un double critère. Nous voulons d’une part
   gagner en connaissance sur le comportement de la fonction et donc
   choisir une zone de l’espace de recherche où l’inconnu est grand :
   c’est l’exploration. D’autre part, nous souhaitons trouver le point qui
   minimise/maximise notre fonction : c’est l’exploitation. Ces deux
   notions sont matérialisées par les indicateurs statistiques cités
   précédemment que sont l’écart-type et la moyenne. Quand l’écart-type
   est grand, c’est que la zone est mal connue et donc intéressante à
   explorer. Quand la moyenne est petite/grande, c’est que la zone
   observée est intéressante pour trouver un minimum/maximum.

   Ce compromis entre exploration et exploitation est exprimé par une
   fonction d’acquisition. Cette fonction associe à chaque point de
   l’espace de recherche un potentiel pour être l’optimal. Il existe
   plusieurs fonctions d’acquisitions, avec des variations subtiles mais
   gardant le même principe directeur : exprimer le potentiel d’un point.
   fonction d'acquisition Upper Confidence Bound

   Upper Confidence Bound est un exemple de fonction d’acquisition, le
   coefficient k exprime le poids donné à l’exploration dans le compromis
   évoqué précédemment µ : moyenne – σ : écart-type


   À chaque étape de notre optimisation, le point choisi pour l’évaluation
   donc est celui qui maximise notre fonction d’exploitation.


   En bleu, notre fonction d’acquisition nous indique les points à plus
   fort potentiel à tester : ils représent le compromis entre exploitation
   (près du minimum) et exploration (dans des zones inconnues)

   Trouver le nouveau point à évaluer implique donc d’évaluer notre
   fonction d’acquisition pour tout notre espace de recherche. Cependant
   ces évaluations seront beaucoup moins coûteuses en comparaison à
   l’évaluation de la fonction observée. La fonction d’acquisition étant
   simple et connue, on peut facilement l’évaluer en un grand nombre de
   points pour trouver le maximum. Le maximum trouvé correspond au
   prochain point à tester et une fois l’observation réalisée, on peut
   actualiser notre optimisation avec cette nouvelle information et
   repartir depuis le début, en répétant jusqu’à converger.

   Pour bien comprendre cela, reprenons notre exemple en voyant après
   plusieurs itérations comment chaque boucle apporte de l’information
   supplémentaire jusqu’à atteindre un minimum. On peut comparer enfin
   l’approximation obtenue à la fonction originale (inconnue à la base) à
   minimiser et vérifier l’efficacité de l’algorithme.

   Le cycle d’optimisation bayésienne à la recherche la valeur donnant le
   résultat minimal sur dix itérations

   Le résultat final après dix itérations avec l’approximation µ(x) faite
   par l’optimisation bayésienne comparé à la “vraie” fonction à
   minimiser, inconnue au départ

   Pour voir cela en pratique, passons à un exemple plus concret !

3 – Et en pratique, comment ça se passe ?

   Maintenant que nous en savons un peu plus sur l’optimisation
   bayésienne, reprenons notre problème initial et voyons comment
   implémenter en Python un algorithme répondant à ce même problème.

   Supposons une situation où nous souhaitons configurer notre base de
   données sur laquelle nous pouvons faire varier deux paramètres param_1
   et param_2. Le but est de déterminer quelles sont les valeurs de ces
   deux paramètres permettant de minimiser le temps pour compléter une
   charge de travail (par exemple un ensemble de requêtes). Les règles
   métier dont nous disposons ne nous permettent pas de déterminer les
   valeurs optimales pour ces paramètres et nous fournissent au mieux des
   bornes dans lesquelles rechercher l’optimum.

3.1 – Une implémentation en Python avec scikit-optimize

   Nous allons ainsi utiliser le package Python scikit-optimize (basé sur
   scikit-learn) pour mettre en place un workflow prenant en entrée un
   intervalle pour nos valeurs de paramètres et cherchant l’ensemble
   donnant les meilleurs résultats. Tout le code du workflow et des
   visualisations est disponible sous forme de notebook Jupyter.

   Commençons par définir notre espace de recherche sous forme de
   dictionnaire :
search_space = {
  'param_1': (0.0, 10.0),
  'param_2': (0.0, 0.0)
}

   Il faut désormais instancier un objet Optimizer, tiré du package
   scikit-optimize, qui va se charger de la majeure partie du travail.
   Nous devons préciser à l’Optimizer l’espace sur lequel il va effectuer
   sa recherche. Comme expliqué précédemment, nous allons utiliser un
   estimateur basé sur les processus gaussiens (ou Gaussian Process,
   “GP”).

   Pour éviter des effets de bord des processus gaussiens survenant
   lorsque l’espace des observations est vide (ou quasi-vide), il est
   aussi préférable de tester les premiers points de manière aléatoire, ce
   que l’on précise par le paramètre n_initial_points.
opt = Optimizer([search_space['param_1'], search_space['param_2']],
                "GP",
                n_initial_points=3)

   Il n’y a maintenant plus qu’à faire tourner la boucle d’optimisation.
   La force de scikit-optimize se situe dans la possibilité de faire
   tourner des workflows relativement asynchrones grâce aux fonctions ask
   et tell. À chaque étape, nous allons demander (ask) quelle est la
   configuration qui présente le meilleur potentiel pour pouvoir la
   tester, c’est-à-dire exécuter notre charge de travail, avant d’obtenir
   un résultat que nous allons ajouter à nos observations (tell). On
   boucle ensuite autant de fois que l’on souhaite ou jusqu’à ce qu’un
   critère de satisfaction soit atteint.
for _ in range(20):
    # Quelle est la prochaine configuration à tester ?
    next_config = opt.ask()  # Renvoie une liste

    # Ici, nous avons une dépendance extérieure puisque nous devons mettre à
    # jour la configuration de la base de données puis tester notre charge.
    # Faisons l'hypothèse que ces interfaces ont été définies ailleurs dans le c
ode.
    database.update_configuration(next_config)
    runtime = database.benchmark(workload)

    opt.tell(next_config, runtime)

   Si tout se passe bien, l’optimiseur convergera ensuite naturellement
   vers un couple de paramètres présentant la meilleure performance.

   Pour se représenter cela, supposons que le temps nécessaire pour
   compléter la charge se comporte de la manière suivante en fonction des
   paramètres param_1 et param_2 :

   Ce comportement est ici hypothétique et l’optimiseur ne le connaît pas.
   Voici comment évolue l’optimisation pour déterminer le minimum à chaque
   itération (pour rappel, le détail est disponible dans ce notebook) :

   L’optimisation bayésienne démontre là tout son intérêt : en un nombre
   d’observations très restreint (de l’ordre de la dizaine), nous sommes
   en mesure de déterminer les valeurs des paramètres pour lesquelles
   notre base de données a les meilleurs résultats. Rapidement,
   l’algorithme converge vers des valeurs autour de (6, 13) en gardant
   tout de même un caractère exploratoire pour s’assurer de ne pas être
   coincé sur un minimum local si le comportement de la base de données
   avait été plus complexe.

3.2 – Gestion du bruit et de la variabilité de l’environnement

   Sur un cas réel, il faut cependant se poser la question du bruit :
   l’optimisation bayésienne est-elle résiliente lorsque les observations
   sont bruitées (dans notre cas, on peut par exemple avoir un temps qui
   varie à cause de perturbations réseaux) ? La réponse est oui, dans une
   certaine mesure. Sur un cas plus compliqué, disponible sur ce notebook,
   on voit qu’un niveau de bruit inférieur à 5% peut perturber
   l’algorithme sans que cela ne soit gênant pour déterminer un minimum
   rapidement. Un autre exemple sur un espace à une dimension démontre
   également cela sur le repository du package scikit-optimize.

   Lorsque le bruit est plus élevé, il est possible de jouer sur des
   paramètres qui peuvent prendre en compte l’incertitude (c’est le cas du
   paramètre alpha de l’estimateur scikit-learn GaussianProcessRegressor).
   Mais lorsque cette variabilité est trop élevée, l’algorithme ne peut
   simplement pas fonctionner correctement. Pour cette raison, il est
   conseillé avant d’utiliser une stratégie d’optimisation bayésienne de
   vérifier que l’environnement est suffisamment stable et que les
   observations sont reproductibles. Après tout, on ne peut pas optimiser
   ce qui varie.

   Des observations trop bruitées ne permettent pas de converger vers un
   minimum : ici, les résultats varient trop pour pouvoir décider quel
   point tester ensuite

3.3 – Application au tuning de modèles de machine learning

   L’exemple précédent d’application de l’optimisation bayésienne est
   relativement précis, mais la stratégie peut facilement se généraliser à
   d’autres applications où l’on cherche un ensemble de paramètres
   maximisant la performance mais que chaque observation est coûteuse.

   C’est notamment le cas de la gestion des hyper-paramètres d’un modèle
   de machine learning, la librairie scikit-optimize est d’ailleurs
   taillée pour ce type d’utilisation. En effet, lorsqu’on utilise un
   modèle de machine learning, il est difficile d’évaluer directement la
   valeur optimale de certains paramètres (par exemple le nombre d’arbres
   et leur profondeur pour un RandomForestClassifier). Utiliser
   l’optimisation bayésienne pour résoudre le problème à notre place est
   tout à fait possible : l’entrée de notre problème correspond aux
   valeurs des hyper-paramètres et la performance à optimiser est une
   métrique au choix de l’utilisateur (précision, AUC, etc.).

   Comment déterminer rapidement les valeurs des hyper-paramètres
   maximisant la métrique de performance ?


   Cette approche permet notamment de gagner en temps d’optimisation en
   réduisant le nombre d’observations par rapport à des algorithmes type
   RandomSearch ou GridSearch qui demandent plus de tests pour arriver à
   des résultats. C’est d’ailleurs l’approche plébiscitée en 2017 par
   Google pour le tuning d’hyper-paramètres qui met même en place un
   système de “hyperparameter tuning as a service” sur Cloud ML.

   Convaincu par l’optimisation bayésienne ? Vous souhaitez optimiser
   rapidement vos modèles de machine learning ? Jetez donc un oeil aux
   solutions prêtes à l’emploi de librairies Python comme scikit-optimize
   ou bayesian-optimization qui présentent toutes les deux des exemples
   d’implémentation appliquée au tuning de modèles ici et ici.

Conclusion

   L’approche bayésienne est donc efficace pour répondre au problème du
   choix de paramètres optimaux lorsque le comportement de la fonction est
   mal connue, que les observations sont coûteuses et qu’une approche
   aléatoire ou brute force n’est pas envisageable. Les algorithmes
   d’optimisation bayésienne se prêtent donc bien à la configuration de
   systèmes d’informations comme une base de données, à condition de bien
   connaître son environnement et sa variabilité !

   Enfin, on a pu récemment voir de grandes entreprises communiquer sur
   des initiatives exploitant ces approches. Des ingénieurs de Twitter par
   exemple, développent un système utilisant l’optimisation bayésienne
   pour configurer automatiquement et en continu les paramètres JVM de
   leurs services. En juin 2018, Facebook a présenté Spiral, une librairie
   destinée à la configuration automatique et temps réel des services
   avec, en préparation, l’utilisation de l’optimisation bayésienne pour
   la configuration de paramètres. Autant d’approches illustrant la force
   de l’optimisation bayésienne lorsqu’on l’applique à la configuration de
   systèmes informatiques.
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans Data Science.

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

   Sécurité

Comment conserver les mots de passe de ses utilisateurs en 2019 ?

   Posté le 02/10/2019 par Fabien Leite
   Mot de passe

   Lorsque vous concevez une application, vous vous posez forcément la
   question de l’authentification et du contrôle d’accès. Pour ça,
   plusieurs méthodes sont disponibles et la première qui vient
   généralement à l’esprit est l’utilisation d’un couple identifiant / mot
   de passe. Dans la mesure du possible, on préfèrera utiliser une
   solution dédiée à l’authentification et au contrôle d’accès : en bref,
   utiliser une solution d’IAM pour gérer ces aspects à votre place. C’est
   généralement plus simple à maintenir et c’est surtout souvent meilleur
   pour l’expérience utilisateur. …
   Lire la suite

   Méthode

Amélioration continue : Comment rester dynamique à mesure que l’équipe
s’agrandit ?

   Posté le 01/10/2019 par Etienne Girot

   Différentes études1 soutiennent qu’une équipe performante est une
   équipe qui est capable de remettre fréquemment en question ses modes de
   fonctionnement afin d’apprendre et s’améliorer en continu. Pour y
   arriver, elle : favorise l'émergence de nouvelles idées a moyen de
   valider ou d’invalider efficacement la pertinence de ces nouvelles
   idées est en mesure d’aligner ses membres derrière les idées retenues
   comme étant pertinentes Or, plus une équipe grandit (aussi bien en
   nombre de membres qu’en temps passé à travailler ensemble) plus elle
   est sujette à…
   Lire la suite

   Méthode innovation

Culture Innov’ : Quel ROI attendu ?

   Posté le 01/10/2019 par Sylvain Fagnent, Matthieu VETTER
   [school.png]

   Après des années et des millions investis sous la menace de la
   disruption, les Directions reviennent à une logique plus ROIste. Ce
   mouvement est sain pour optimiser les ressources rares et donner du
   sens au travail des innovateurs. Le risque est cependant de tuer dans
   l’oeuf des pépites potentielles en pilotant l’innovation comme un
   business opérationnel. En fonction des objectifs, les retours sur
   investissement (ROI) d’une démarche d’innovation seront différents. Et
   dans un monde de plus en plus focalisé sur la rentabilité à court
   terme,…
   Lire la suite

   Data Science

Ouvrir la boîte noire et comprendre les décisions des algorithmes

   Posté le 30/09/2019 par Annabelle Blangero
   [school.png]

   L’usage des algorithmes de traitement de données – de la simple requête
   SQL aux puissants algorithmes de recommandation et de personnalisation
   des géants de la Tech – s’est popularisé ces dernières années,
   notamment pour des utilisateurs traditionnellement hors du domaine IT.
   Cet usage se retrouve dans tous les secteurs (industrie, éducation,
   santé, sécurité, etc.) et tend à déléguer de plus en plus de décisions
   à des systèmes automatisés. Cette appropriation par le plus grand
   nombre rend les naufrages encore plus probables, et l’exemple de
   Cambridge…
   Lire la suite

   Archi & techno

Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos valeurs et le
garant de la vision technique d’OCTO.”

   Posté le 27/09/2019 par Joy Boswell
   [archi.png]

   Chez OCTO depuis plus de 10 ans , Meriem fait partie des personnes
   fondatrices de l’entreprise. Ancienne leadeuse de la tribu Nouvelles
   Architectures de Données, elle est désormais CTO et participe à la
   définition de la vision stratégique et technique d’OCTO. Qui de mieux
   pour nous parler du “tech leading à la OCTO” ?
   Lire la suite

   Data Science

Mise en application de DVC sur un projet de Machine Learning

   Posté le 27/09/2019 par Nicolas TOUSSAINT, Jérémy Bouhi
   [school.png]

   Introduction DVC (Data Version Control) est un package Python qui
   permet de gérer plus facilement ses projets de Data science. Cet outil
   est une extension de Git pour le Machine Learning, comme l’énonce son
   principal contributeur Dmitry Petrov dans cette présentation. DVC est à
   la fois comparable et complémentaire à Git. Il va s’occuper de
   synchroniser vos données et votre code. Il est donc particulièrement
   intéressant dans le cadre d’un projet de Machine Learning où le modèle
   et les données évoluent au fil du développement.…
   Lire la suite

   Archi & techno

BD – Le Déploiement Continu (CD)

   Posté le 26/09/2019 par Aryana Peze
   [archi.png]

   Hello ! Lors de la BD précédente, nous avons abordé le sujet de la CI
   (Intégration Continue). Et impossible de parler de CI sans parler de CD
   (Déploiement Continu)! En thoérie, la CD implique un déploiement
   automatique et quasi-systématique de chaque modification du code sur
   l'environnement de production. Les mises en production sont régulières
   et ne sont plus une source de stresse, et l'environnement de production
   est ainsi toujours à jour. En pratique, c'est un objectif très
   compliqué à atteindre, et pas toujours adapté. (Petite parenthèse…
   Lire la suite

   Agile

D’étudiant à mentor : rencontre avec notre Octo Thomas Le Flohic

   Posté le 24/09/2019 par Céline Audibert
   [agile.png]

   Thomas a fait un véritable parcours “à la OCTO” : après son école
   d’ingé, il rentre en stage au sein de notre tribu VIBE (Virtual
   Immersion and Bot Experience) et rejoint définitivement l’entreprise en
   intégrant le programme Skool. Un de ses profs à l’école était un Octo,
   Fabien. C’est ce qui lui a donné envie de venir frapper à notre porte.
   Comme lui, Thomas a voulu garder un lien avec l’école et transmettre
   son savoir. C’est ainsi qu’il s’est lancé dans l’accompagnement d’un
   projet de…
   Lire la suite

   Archi & techno

Interview Céline Gilet – « Le Tech Lead n’est pas un super héros ! »

   Posté le 23/09/2019 par Céline Audibert
   [archi.png]

   Depuis plus de 4 ans chez OCTO, Céline, membre de la tribu CRAFT, est
   devenue une référence parmi nos Tech Lead. Découvrez sa vision de ce
   rôle à part. Pour toi, quel est le rôle du Tech Lead ?  Pour moi, c’est
   faire en sorte que l’équipe au sens large (Développeurs, Ops,
   Fonctionnels, Product Owner) arrive à délivrer régulièrement de la
   valeur. Concrètement, il s’agit de jongler et prioriser en permanence
   entre plusieurs casquettes : expertise, accompagnement, coaching et
   formation.
   Lire la suite

   Méthode innovation

Injonctions paradoxales : un MVP … mais pour tous !

   Posté le 20/09/2019 par Dominique Lequepeys, Sylvain Fagnent
   un MVP pour tous

   Un MVP ... mais pour tout le monde ! Et si vous vouliez l’entendre
   cette injonction ? Mise en scène, écoutez la. Les racines du paradoxe
   D'un côté, les managers sont sous la pression du timing : ils sont
   séduits par le concept de MVP, Minimum Viable Product, présenté comme
   un moyen d'accélérer la mise sur le marché. D'un autre, ils sont sous
   la pression du chiffre : ils ont du mal à accepter qu'on se prive d'une
   partie du marché potentiel. En outre, ils…
   Lire la suite
   1234>
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » Le chemin vers l’omnicanal Flux des commentaires Le
   demi-cercle (épisode 48 — Plaques tournantes) L’optimisation bayésienne
   par l’exemple : à quoi ça sert et comment ça marche ? alternate
   alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

Le chemin vers l’omnicanal

   Posté le 01/08/2018 par Julien Kirch
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Si votre système d’information n’est pas tombé dedans quand il était
   petit, faire de l’omnicanal est souvent un parcours semé d’embûches, et
   de promesses d’éditeurs.

   Cet article se propose de parler des sujets de fond liés à ce type de
   transformations.

   Il est aussi l’occasion d’aborder l’histoire des SI pour comprendre
   comment on est arrivé à la situation actuelle.

C’est quoi l’omnicanal ?

   Un canal est un point d’accès spécifique à un système. Un SI peut par
   exemple avoir un canal web client, un canal application mobile client,
   un backoffice de gestion, des accès pour les partenaires…

   Un SI omnicanal est un SI qui permet aux différentes personnes qui
   l’utilisent de passer de manière fluide d’un canal de distribution à un
   autre.

   L’exemple type est de commencer à faire une demande de prêt immobilier
   sur son smartphone, de la continuer sur son ordinateur portable une
   fois chez soi, puis éventuellement de terminer le dossier en se rendant
   en agence. Cela ne signifie pas utiliser le même outil dans les trois
   cas, mais bien d’avoir des outils spécifiques dont l’ergonomie est
   adaptée à chaque utilisation, et de pouvoir passer de l’un à l’autre
   « sans couture » (seamless), c’est-à-dire sans avoir à refaire une
   opération déjà faite comme une saisie de données.

   En plus des avantages pour les utilisateur·rice·s, nous allons voir que
   l’omnicanalité a également des avantages pour l’IT.

   À l’inverse, un SI multicanal fournit les mêmes fonctionnalités sur
   plusieurs canaux mais de manière silotée, et la bascule d’un canal à
   l’autre est donc visible de la part des personnes ou des systèmes qui
   l’utilisent.

   Dans ce cas une personne qui commence une demande de prêt immobilier
   chez elle et qui se rend en agence devra reprendre le dossier depuis le
   début car le backoffice de l’agence n’a pas accès à la demande faite
   sur le canal web client.

Pourquoi c’est compliqué ?

   Le chantier de transformation omnicanal comporte plusieurs axes, liés
   aux différentes contraintes des systèmes actuels.

   Pour comprendre la situation, le mieux est de revenir en arrière et de
   dérouler l’historique du SI.

   Nous allons prendre ici un exemple typique du domaine bancaire tel
   qu’on le retrouve chez de nombreux acteurs historiques.

Les années 80 : au commencement étaient le mainframe et le backoffice

   Les premières briques du SI se sont construites dans les années 80 sur
   mainframe, développées en COBOL ou équivalent. Ces systèmes historiques
   peuvent être des développements « maison », des progiciels, ou un
   mélange des deux.

   Les écrans de backoffice permettant d’y accéder sont conçus pour les
   employé·e·s de l’entreprise et leurs sont réservés.

   Les workflows de traitement et les fonctionnalités exposées sont
   directement calqués sur les écrans et chaque étape du process est
   stockée dans la base de données d’une manière structurée. Il ne s’agit
   pas d’un modèle MVC : les définitions des écrans sont imbriquées avec
   les traitements métiers.

   Les bureaux étant fermés la nuit et le week-end, les appels interactifs
   sont désactivés pendant ces périodes, ce qui permet d’exécuter des
   traitements de masse ou batch. Ces traitements bénéficient ainsi de
   l’intégralité de la puissance de calcul, et le fait d’être les seuls à
   s’exécuter leur permet de simplifier leur design car ils peuvent ainsi
   monopoliser des ressources comme des tables de bases de données sans se
   soucier du reste du monde.

   Cela permet aussi de simplifier les règles métier, par exemple les
   calculs comptables sont beaucoup plus simples lorsqu’aucune autre
   opération n’est effectuée pendant qu’ils s’exécutent.

Les années 2000 : l’arrivée du web, le bicanal

   Avec l’arrivée du web, il est temps d’ouvrir un site de banque en
   ligne.

   Cela signifie donner accès à des fonctionnalités du mainframe, mais
   d’une manière différente de celle du backoffice :
     * les écrans doivent être adaptés pour être utilisables par des
       non-employé·e·s, certains workflows comportent donc plus d’étapes ;
     * certaines options nécessitant la validation d’un·e employé·e
       empêcheront d’aller jusqu’au bout du traitement à partir du site,
       cela nécessitera des opérations de backoffice spécifiques.

   Le système mainframe historique est vital pour l’entreprise et la
   maîtrise qu’ell en a n’est pas toujours satisfaisante : ce patrimoine
   commence à dater et la connaissance s’est donc perdue, il comporte
   rarement des test automatisés et avec une documentation souvent
   lacunaire.

   La stratégie choisie est donc souvent de limiter au maximum l’ampleur
   des modifications sur cette partie du système pour limiter les risques.

   L’approche choisie alors consiste à exposer les workflows existant
   autant que possible – c’est-à-dire ceux du backoffice – principalement
   sous formes d’API synchrones, et à développer le site web au-dessus de
   ces API, alors même que les workflows ne sont pas les mêmes.

   Les contrats de ces APIs sont donc assez proches des écrans mainframe
   pour limiter l’effort à fournir. Il ne s’agit bien entendu pas d’API
   REST, mais généralement de messages MQ ou d’appels CTG.

   Lorsque les deux workflows ne correspondent pas, on aboutit à ce type
   de situation :

   Il est dans ce cas impossible de stocker les résultats des étapes 1A ou
   2A dans le mainframe. Ils seront donc stockés dans le backend du site
   web dans une base de données séparée. Cela signifie aussi qu’il faudra
   dupliquer les contrôles de saisie de ces étapes dans la partie web,
   pour éviter d’avoir à revenir en arrière dans les écrans du site web.

   Suivant les étapes, les données sont donc stockées soit dans le système
   cœur, soit de manière intermédiaire dans le sous-système du site web.

   En fonction des situations, les points de « rencontre » des workflows
   sont plus ou moins nombreux. Le cas extrême est celui où il existe un
   seul point de synchronisation : la dernière étape du workflow. Dans
   cette situation, le site web doit stocker toutes les données
   intermédiaires, et recoder tous les contrôles de saisie.

   Dans ce cas, les données dans la base du site web qui n’ont pas été
   déversées dans la base du mainframe ne sont ni visibles depuis le
   backoffice ni des autres systèmes qui exploitent cette base.

   Par exemple, si vous commencez à souscrire un prêt immobilier sur le
   site web sans terminer la procédure et que vous vous rendez dans votre
   agence bancaire, il faudra refaire tout ou partie des opérations.

   Par ailleurs, les opérations de backoffice spécifiques au site web
   ainsi que les besoins de support client nécessitent de développer des
   écrans spécifiques branchés sur le même backend.

   L’inaccessibilité du cœur système historique pendant la nuit pose aussi
   problème : il est inconcevable de faire de même pour un site web
   destiné au grand public.

   Il existe de nombreuses manières d’améliorer cette situation,
   l’approche souvent rencontrée consiste à :
    1. effectuer une copie de certaines données avant de couper le système
       mainframe, et s’en servir comme d’un cache en lecture seule
       accessible pendant la nuit, le cache sera désactivé lorsque les
       traitements de masse sont terminés ;
    2. ne pas exécuter les opérations qui nécessitent des écritures mais
       les enregistrer sous forme de demandes d’exécutions dans le backend
       du site web, et réaliser réellement les traitements le jour suivant
       à l’ouverture du mainframe.

   Cela rend le SI plus difficile à observer car les données sont
   distribuées entre les deux sous-systèmes.

   Bien entendu, même si la réutilisation de fonctionnalités existantes
   est privilégiée, certains besoins du site web nécessitent de développer
   des APIs spécifiques dans le cœur métier.

Aujourd’hui : le mobile et les partenaires

   L’arrivée du mobile pourrait signifier la mise en place d’une
   tricanalité. Mais les besoins mobiles sont souvent suffisamment proches
   des besoins web pour qu’ils s’appuient sur les mêmes systèmes. Dans
   quelques situations, il peut être nécessaire de stocker des données
   intermédiaires sur les terminaux, mais il ne s’agit pas d’un vrai
   troisième canal.

   Les écrans de backoffice ont souvent été remplacés par des technologies
   web. Mais pour limiter les impacts sur le mainframe, on conservera
   souvent les mêmes workflows, le nouveau backoffice n’aura donc pas à
   stocker de données.

   De même, le site web public a pu être refondu, mais toujours en
   subissant les contraintes de l’existant.

   En revanche, la banque a noué des partenariats. Ces partenaires peuvent
   par exemple vendre des prêts de la banque en marque blanche quand vous
   achetez un de leur produits.

   Les process nécessaires aux partenaires sont aussi différents du
   process historique que du process web, le système devient donc souvent
   tricanal. Prenons le cas où l’intégration se fait via un backend
   spécifique.

   Pour rester lisible, le schéma ne contient pas les backoffice dédiés
   aux canaux web et partenaires mais ils existent bel et bien, une
   personne du support peut donc avoir à jongler avec trois backoffices
   différents.

   Le canal partenaire ne pose pas le même problème que le canal web. En
   effet, un client qui commence à souscrire un prêt en marque blanche en
   achetant un bien voudra rarement conclure la transaction dans votre
   agence. En revanche, la multiplication des canaux rend la maintenance
   du système plus complexe quand on veut modifier un des workflows
   centraux qui sont exposés aux autres canaux ou changer une des règles
   de gestion dupliquée à plusieurs endroits.

   Certains besoins des partenaires se rapprochent de ceux du site web
   client, il arrive donc qu’une partie du code soit partagée entre les
   deux. Cela évite de développer plusieurs fois les mêmes choses mais
   rend le système encore plus difficile à observer.

En résumé : les problèmes du multicanal

   Le multicanal pose donc les problèmes suivants :
     * mauvaise expérience utilisateur·rice·s lors du passage d’un canal à
       l’autre ;
     * duplication de code entre les canaux ;
     * données partiellement dupliquées entre les canaux ;
     * limites dans la capacité à créer des parcours très différents du
       parcours historique ;
     * difficulté de mettre en œuvre des évolutions cross-canaux du fait
       de la duplication ;
     * système difficile à observer.

Que faut-il pour avoir un SI omnicanal ?

   Les problèmes causés par le multicanal et les limites des SI
   correspondants nous donnent les informations nécessaires pour dresser
   le plan d’un SI omnicanal.

   Avant de rentrer dans le détail, il faut préciser qu’un système
   omnicanal ne signifie pas un système unique de haut en bas pour tous
   les canaux mais un système cœur permettant de répondre aux besoins de
   l’omnicanalité sur lequel viendront se brancher les différents canaux.

   La différence avec un système multicanal est la capacité de passer d’un
   canal à l’autre, pas le fait d’avoir un système unique.

   Ainsi vous n’exposerez pas forcément les mêmes services ou les mêmes
   technologies pour votre application mobiles et pour vos partenaires.
   Vous aurez un système cœur sur lequel viendront se greffer votre canal
   backoffice, votre canal public, votre canal partenaire…

Des processus métier indépendants des canaux

   Les workflows étant différent d’un canal à l’autre, l’omnicanalité
   nécessite de concevoir des processus métier qui soient adaptables aux
   différents canaux.

   Cela signifie qu’il ne faut pas penser son processus en termes d’étapes
   qui ont la granularité d’un écran mais en termes de macro-étapes avec
   une taille plus importante, ce qui donnera à chaque canal les marges de
   manœuvres dont il a besoin.

   Par exemple, souscrire un crédit peut, en le simplifiant à l’extrême,
   se décomposer en trois macro-étapes :
     * renseigner des informations personnelles et faire des simulations
       de crédit jusqu’à obtenir une offre satisfaisante ;
     * valider une demande de crédit en saisissant des informations
       supplémentaires ;
     * traiter la demande dans le backoffice pour la valider ou la
       rejeter.

   Il s’agit d’un travail de conception métier. C’est souvent la partie la
   plus difficile du chantier car il s’agit d’un exercice dont on a peu
   l’habitude, et c’est donc une bonne première étape.

Un système de stockage

   Les données doivent être stockées dans un système indépendant des
   canaux.

   Comme les saisies d’informations peuvent se faire dans des ordres
   différents d’un canal à l’autre, on peut moins souvent s’appuyer sur
   des contraintes d’intégrité que dans un système monocanal.

   Par exemple un·e client·e pourra peut-être créer un compte sans fournir
   immédiatement son nom ou son adresse.

Des règles métier de validation

   Dans un système historique, les services métier étant adossés aux
   écrans, chacun comportait les règles métiers correspondantes permettant
   de valider les informations saisies dans le formulaire.

   Dans un système omnicanal, ce n’est plus possible car chaque canal peut
   concevoir son parcours.

   Cela signifie que les règles de validation seront sous deux formes :
    1. dans le système central, des règles de validation seront placées au
       niveau de chaque macro-étape ;
    2. les canaux doivent implémenter ces mêmes règles au niveau de chaque
       écran ou de chaque service exposé avec la granularité la plus fine
       possible pour être en mesure de remonter des erreurs au plus près
       de la saisie des données.

   Cela nécessite de bien documenter les règles.

Des services facilement utilisables et composables

   Ce sont les services synchrones et asynchrones sur lesquels seront
   construits les canaux.

   En effet, composer des services pour de l’omnicanal signifie de bien
   maîtriser les dépendances entre les différents services pour donner des
   libertés aux différents canaux.

   Ces services doivent aussi, autant que possible, être accessibles 24
   heures sur 24. Cela va nécessiter, du point de vue de l’extérieur, que
   les traitements ensemblistes « de nuit » ne rendent plus le système
   inaccessible. Cela peut demander de réutiliser le même type de
   comportements que ceux qui étaient utilisés par les canaux, comme le
   fait d’enregistrer des demandes d’exécutions à traiter plus tard. La
   différence est que le comportement sera cohérent entre les différents
   canaux car réalisé dans la partie commune.

Les canaux

   C’est la partie spécifique à chaque canal qui définit le workflow de ce
   canal et l’expose de la manière appropriée par des écrans ou des
   services.

   L’objectif est que cette partie du SI ne stocke pas d’information. En
   effet, comme nous l’avons vu plus haut, toute information stockée au
   niveau d’un canal va créer un silotage. Ils ne font que s’appuyer sur
   les services de la couche cœur.

   L’omnicanalité rend la conception des canaux plus difficiles car ils
   doivent prendre en compte le fait qu’un processus peut avoir été
   démarré dans un autre canal ayant un workflow différent.

   Par exemple, certains des champs de saisie auront peut-être déjà être
   remplis et pas d’autres.

   Il faut qu’il puisse déterminer comment effectuer la reprise du
   traitement dans de bonnes conditions.

   Cela demande une conception rigoureuse ainsi qu’une bonne couverture de
   tests.

Faire vivre le système

   La dernière pierre de l’omnicanal est la capacité à le faire vivre.

   En effet, les canaux sont fortement couplés au système cœur, ils
   devront donc être modifiés de manière coordonnée.

   Ce couplage est un effet direct de l’omnicanalité : c’est elle qui
   permet de passer d’un canal à l’autre. Le modèle de canaux découplés
   est celui du multicanal.

   Votre organisation doit donc être adaptée à cette contrainte.

Comment y aller ?

   Maintenant que nous savons en quoi devrait consister un système
   omnicanal, reste à étudier les trajectoires pour l’atteindre.

   Nous allons commencer par un point sur la situation de départ puis
   donner quatre exemples de stratégie possibles. Il existe de multiples
   approches, celles qui sont mentionnées ici ont été choisies car elles
   mettent en lumières les contraintes qui s’appliquent.

Situation de départ

   Le système multicanal comporte deux éléments qui ont de la valeur et
   sur lesquels il faut s’appuyer en le faisant évoluer vers l’omnicanal,
   et deux limites qu’il faudra supprimer :

   À conserver :
     * les règles de traitement métier ;
     * les règles de validation de données.

   Les deux représentent de la valeur même si elles sont adhérentes au
   étapes du workflow historique (par exemple les différents écrans du
   process de souscription originel).

   À supprimer :
     * le workflow unique formant l’assise du système historique
     * les règles d’intégrité des données alignées avec le process
       historique

Stratégie 1 : commencer par acheter un BPM

   C’est la solution que préconisent certains éditeurs.

   Les BPM sont des outils permettant de définir des workflow métiers sous
   forme low-code, c’est-à-dire via de la configuration et/ou des
   designers graphiques. Ils permettent également de stocker l’état
   courant des différents workflows.

   C’est une solution tentante car elle fournit un socle prêt à l’emploi
   pour une partie des besoins.

   Deux points d’attention pour cette approche :
     * comme avec tout progiciel, attention à ne pas oublier les bonnes
       pratiques de développement comme les tests automatisés : votre BPM
       embarquera du code, et qui dit code dit tests ;
     * ne pensez pas qu’avoir choisi un BPM signifie que vous avez gagné,
       en effet nous avons vu que la partie la plus difficile du chantier
       est la conception des services sur lesquels va s’appuyer le BPM.

   Il s’agit d’une utilisation très spécifique des outils de BPM, loin de
   la gestion des processus métiers qui est leur utilisation normale.

Stratégie 2 : repartir sur un nouveau système

   C’est la solution la plus risquée, mais qui est parfois la moins
   mauvaise. Par exemple quand vous avez perdu la maîtrise de votre
   système historique, ou qu’il s’agit d’un progiciel qui n’est pas
   compatible avec l’omnicanal.

   La solution n’est pas forcément de partir de zéro : il est possible de
   partir sur un progiciel plus récent, ou de racheter une entreprise
   disposant d’une solution déjà fonctionnelle.

Stratégie 3 : rendre le cœur métier historique omnicanal

   Il s’agit d’attaquer le problème par le bas, c’est-à-dire par le cœur
   métier.

   Cela peut être à l’occasion de l’ajout d’un nouveau canal, en profitant
   d’avoir des nouveaux besoins factuels, et un budget.

   Il va s’agir de transformer le cœur, puis de faire maigrir les canaux
   existants en redescendant ce qui ne devrait pas s’y trouver, comme le
   stockage de données.

   La situation de départ


   En cours de migration : les canaux diminuent et le cœur s’enrichit


   Cible : les canaux n’ont plus de base de données

   C’est probablement la meilleure solution si vous avez la maîtrise de
   votre existant et que vous souhaitez capitaliser dessus.

   Deux points d’attention :
     * faire évoluer de manière significative un outil demande un niveau
       de maîtrise plus important que le fait de le maintenir, la facilité
       à corriger des erreurs sur le cœur n’est pas un bon indicateur de
       votre capacité à le transformer ;
     * ne pas introduire de régressions, par exemple en supprimant des
       comportements non documentés mais sur lesquels le code s’appuie.

Stratégie 4 : ajouter une couche d’omnicanal au-dessus du cœur

   Il s’agit de la voie intermédiaire : on s’appuie sur l’existant le
   temps de bâtir un remplacement.

   Il s’agit de bâtir une surcouche omnicanale au-dessus du cœur. Plutôt
   que de partir de zéro, il est possible de partir d’un des canaux
   existants en le séparant entre une partie souche qui servira de base à
   la partie omnicanal et la partie exposition qui deviendra la nouvelle
   couche canal.

   En enrichissant peu à peu de nouveau types de données en les remontant
   depuis le cœur historique et des fonctionnalités associées. Cette
   couche devra exposer les services réutilisables qui serviront de base
   aux différents canaux.

   Pendant la construction, vous continuerez de subir les limitations du
   cœur existant, mais commencerez à bénéficier de certains avantages de
   l’omnicanalité, comme la transition plus facile d’un canal à l’autre.

   L’étape suivante consistera à dégonfler le système historique pour
   s’appuyer de plus en plus sur la nouvelle couche.

   Cela va probablement demander des évolutions du système cœur. Cependant
   elles ne demanderont pas de transformations profondes, au contraire de
   la stratégie précédente.

   En cible on pourra décomissionner totalement le système historique, ou
   conserver certains éléments comme les parties réglementaires pour
   lesquels la migration ne se justifie pas et qui n’imposent pas de
   contraintes sur le nouveau système.

   Une des difficultés de cette stratégie est de bien choisir l’ordre dans
   lequel remonter les fonctionnalités pour bénéficier au plus vite des
   premiers avantages tout en limitant les risques.

   La situation de départ

   En cours de migration, la zone du milieu prend de l’importance

   Cible : le cœur historique n’est plus le centre du système

Pour terminer

   L’omnicanalisation d’un SI est un chantier risqué et de longue haleine.
   Mal conçu ou mal piloté, il peut être un enfer de plusieurs années qui
   aboutira à ajouter de nouvelles briques à votre système, sans atteindre
   aucun des buts fixés.

   Il est autant lié à la DSI qu’au métier : il demande du travail à tous
   les deux, mais apportera aussi des avantages à chacun. Si l’un des deux
   acteurs veut se lancer sans la pleine coopération de l’autre, c’est
   l’échec presque assuré.

   Même si ce changement peut permettre de réduire la dépendance aux
   systèmes historiques, y arriver va demander de comprendre comment ces
   systèmes fonctionnent, et de les modifier. Moins bien vous maîtrisez
   votre mainframe, plus il sera difficile de vous en passer.

   Si un tel projet vous semble long et coûteux aujourd’hui, gardez à
   l’esprit que plus le temps passe et plus la situation va empirer.

   Bonne chance à vous.
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans Archi & techno, Stratégie SI et taggué
   Architecture SI, omnicanal.

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Un commentaire sur “Le chemin vers l’omnicanal”

     Anas
   02/08/2018 à 16:20
   Une stratégie qui n'a pas été évoquée et qui mérite de l'être, il
   s'agit de la solution CEP couplée à une solution de suivi de parcours.
   La solution BPM n'est pas très adaptée à des parcours impliquant des
   millions d'utilisateurs car il faut pouvoirs stocker et gérer autant
   d'instances de processus que de parcours utilisateurs en cours. Pour
   peu que le parcours dure quelques jours voir quelques semaines (ce qui
   est le cas dans le domaine bancaire), on atteint très vite des volumes
   difficilement supportables par les solutions BPM du marché.

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha

   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » Le demi-cercle (épisode 49 — Cocktail) Flux des
   commentaires L’optimisation bayésienne par l’exemple : à quoi ça sert
   et comment ça marche ? GraphQL: Et pour quoi faire ? alternate
   alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

Le demi-cercle (épisode 49 — Cocktail)

   Posté le 03/08/2018 par Christophe Thibaut
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   The world is not interested in the storms you encountered, but did you
   bring in the ship?
   William McFee

   Sept heure moins dix. Tu entres dans la Grande Salle de la Direction
   Générale. Toutes les lumières sont allumées bien que le jour soit
   encore clair. On a plié toutes les tables sauf une, et repoussé les
   chaises dans un coin.

   Pop !

   Victor sert le champagne dans des flûtes. Tu te demandes si elles ont
   été louées pour l’occasion, ou si décidément cette maison regorge de
   trésors cachés. Il y a du monde, au moins 40 personnes. Audrey et
   Jérémie sont en train de discuter avec un homme en costume trois pièces
   et une jeune femme d’apparence assez chic malgré une chevelure hirsute.
   Certainement des clients de chez Juniper. Tu t’approches. Audrey dit :
   – En tout cas, c’est une bonne chose que vous ayez pu rencontrer
   Victor. J’étais loin de maîtriser mon sujet sur Parady !
   La jeune femme dit :
   – Vous vous en êtes très bien sortis. Une présentation très
   professionnelle, vraiment.
   Jérémie sourit d’un sourire que tu ne lui connais pas, et répond :
   – Merci. Beaucoup.
   L’homme en costume dit :
   – Et surtout, ça nous a permis de mieux faire connaissance avec votre
   produit. Ce qu’on ne vous a pas dit, c’est que ces features que vous
   nous avez présentées, on les cherche depuis deux ans sur le marché !
   Une chance que Victor se soit planté en scooter.
   Mais non, qu’est-ce que tu racontes.
   Il aurait pris tout l’espace et le client n’aurait pas vu la démo
   d’Audrey et Jérémie, et il ne nous aurait pas acheté la gamme complète.
   La médisance…

   Farid et Hugo sont arrivés. Ils serrent des mains. Prennent une flûte.

   Tintement de verre. Les conversations s’estompent.

   Le Président Directeur Général, Gérard Beaufret, lève son verre un
   instant, le pose sur la table derrière lui, et déclare :
   – Je voudrais remercier d’abord nos clients, anciens et nouveaux (il
   adresse un sourire au couple venu de Juniper), merci, pour la confiance
   que vous nous faites, et qui nous honore. Nous cherchons constamment à
   améliorer la qualité de nos produits, et vos appréciations — et bien
   entendu vos critiques — nous sont très très utiles.

   Bien entendu.
   Stop.

   Le PDG reprend :
   – Je voudrais remercier l’équipe XXL…
   Il fait un geste pour vous inviter Audrey, Jérémie, Hugo, Farid et toi
   à entrer dans le grand cercle. Vous avancez. Tu souris.

   Tu souris trop.
   Mais non.

   – … sans qui cette épopée ne serait pas possible. Chapeau ! Et sans
   oublier bien sûr, notre chère Maria, et Victor qui reprend le flambeau
   de plus belle.

   Tout le monde applaudit, y compris vous. Tu applaudis tes coéquipiers.
   Maria applaudit. Victor applaudit. Jean-Bernard applaudit. Lazare lève
   sa flûte de champagne et adresse un clin d’œil à Jérémie.

   Pas facile d’applaudir avec une flûte à la main.
   Même vide.

   Une bonne demi-heure se passe. Brouhaha de discussions animées, sur
   l’avenir du logiciel, la digitalisation, la vitesse, la réactivité.
   Jérémie, Farid et toi, vous parlez d’architecture. Victor, accompagné
   d’un collègue de la Direction des Ventes, s’immisce dans le groupe et
   le présente :
   – Vous connaissez Daniel Derby ? Product Manager de la gamme Parady.
   Jérémie dit : salut Daniel.

   Bon sang Jérémie connaît tout le monde dans cette boîte.

   Vous vous saluez. La conversation fait comme une pause, puis reprend :

   Daniel : Alors Jérémie, dis nous, c’est quoi votre secret de
   fabrication, chez XXL ?
   Jérémie : Ah ah. Le secret… Mais il n’y a pas de secret.
   Daniel : Sans blague. Regardez le chemin parcouru en 10 mois. Vous avez
   forcément un truc. Comment faites-vous pour aller si vite ?
   Farid : Oui, tiens c’est vrai, c’est quoi notre secret ?
   Jérémie : Pffff. Non, je ne vois pas.
   Victor (s’esclaffant) : Il faut continuer à le faire boire si tu veux
   découvrir le truc. Je vais chercher du champagne.
   Daniel : Si tu y réfléchis, là comme ça ? Qu’est-ce que vous avez de
   différent ?
   Toi : On a un bureau qui donne sur le sud ? Non, je plaisante.

   Victor revient avec du champagne et s’avance pour remplir vos flûtes.
   Jérémie, Farid et toi, déclinez poliment.

   Jérémie (tend son verre) : Oh, remarque après tout. Merci.

   Jérémie te regarde une seconde, déguste son champagne et reprend :
   Jérémie : Notre secret, c’est la qualité des conversations.
   Farid (claque des doigts) : Voilà. Exactement !
   Daniel : La qualité de vos conversations ? C’est une blague ?
   Jérémie : Mais non, pourquoi ? Qu’est-ce qui te fait rire là-dedans ?
   Victor : Remarque qu’il n’a pas tort. Au début ça m’a surpris, mais en
   fait il a un peu raison.
   Daniel : Mais, vous n’êtes pas là pour avoir de jolies conversations de
   salon ! Vous êtes là pour faire votre travail.
   Jérémie : Bien sûr. Moi-même, je n’aime pas les conversations de salon
   comme tu dis. Non, je te parle des conversations au travail. Celles qui
   permettent d’obtenir des informations et de prendre nos décisions.
   Daniel : Ha ! Vous prenez des décisions ! Et combien de décisions vous
   prenez de cette manière ?
   Jérémie : Je dirais plusieurs dizaines par jour. C’est principalement
   ce en quoi consiste notre travail d’ailleurs.
   Farid : Une longue série de décisions…
   Toi : Plus ou moins funestes…
   Farid (souriant) : Tu vois le verre à moitié vide…
   Daniel (vide sa flûte, croise les bras) : Tout de même. Je suis curieux
   de savoir quel type de conversations vous tenez, et à propos de quoi ?
   Jérémie : Ça dépend pas mal de ce que l’on est en train de faire, note.
   Daniel : Ah oui ?
   Jérémie : Mais si on voulait les catégoriser par contenu, je pense
   qu’on trouverait cinq catégories distinctes :
   (Jérémie compte sur ses doigts) Les conversations avec le code. Les
   conversations à propos du code. Les conversations à propos du problème
   que nous voulons résoudre grâce au code. Les conversations à propos de
   notre façon de résoudre le problème
   Toi : Ça fait quatre.
   Jérémie (souriant) : Je sais encore compter. Et puis les conversations
   à propos de toutes ces conversations.
   Daniel : Attends, j’ai perdu le fil. Qu’est-ce que tu appelles une
   conversation avec le code ?
   Jérémie : Tu lis un morceau de code, tu ne le comprends pas bien;
   disons que sa forme est difficile à comprendre. Tu écris un test sur ce
   code, qui te révèle une nouvelle information, ou bien qui confirme une
   hypothèse. C’est comme si tu posais une question. Quand tu as
   suffisamment de tests, tu peux changer la forme du code, ce qui le rend
   plus facile à comprendre. Et tu relances tes tests. C’est ce que
   j’appelle une sorte de conversation. Si on veut.
   Daniel : Ça me rappelle mes études. C’est ce qu’on faisait avec
   Smalltalk. Sauf qu’on ne faisait pas de tests comme vous, on utilisait
   un débogueur intégré.
   Jérémie : Je ne connais pas Smalltalk, mais je dirais qu’on peut faire
   ça avec n’importe quel langage, du moment qu’on a un outil de test et
   un débogueur.
   Daniel : Bon. Admettons. Et les autres conversations ?
   Jérémie : Les conversations à propos du code, sont celles que nous
   avons très souvent entre développeurs. Est-ce que le code fait ce qu’il
   est supposé faire ? Est-ce que le code est facile à comprendre ?
   Qu’est-ce qu’il faut changer dans ce code ? Ce genre de questions.
   Daniel : Je vois. Et ensuite ?
   Jérémie : Les conversations à propos du problème, sont celles que nous
   avons avec Maria, Charlène, Victor et d’autres personnes, et ces
   conversations tournent autour des utilisateurs, des clients et de ce
   qu’ils essayent d’obtenir au moyen d’XXL, et comment ils l’obtiennent.
   Daniel : OK. C’est la partie fonctionnelle.
   Jérémie : Si tu veux.
   Daniel : Mais ça, ça ne devrait pas faire l’objet de conversation
   justement : ça devrait être dans la spécification.
   Jérémie : Ah bon ?
   Daniel : Bien sûr. Pour écrire un programme qui fait correctement ce
   qui est attendu fonctionnellement, il faut des spécifications.
   Jérémie : OK. Je ne dis pas non. Dans ce cas, cette catégorie de
   conversation inclura probablement les conversations à propos de la
   spécification.
   Daniel : Eh bien non, pas du tout. La spécification est justement là
   pour éviter d’avoir à revenir sur le sujet.
   Jérémie : Pour que ce que tu dis soit exact, à savoir, qu’on aie pas
   besoin de revenir sur le sujet, il faudrait que la spécification soit
   complète.
   Victor : Ah ça…
   Jérémie : Note que si la spécification était complète, on pourrait
   probablement la transformer automatiquement en logiciel. On n’aurait
   plus besoin de la traduire en code.
   Daniel : Ha ! C’est ça qui serait pratique !
   Victor : Mais alors les développeurs n’auraient plus de travail… Et
   alors tu ferais quoi, comme métier, Jérémie ?
   Jérémie : Probablement un métier qui tourne autour de la création de
   spécifications.
   Daniel : Bon. D’accord. Ensuite, tu as parlé des conversations à propos
   de notre façon de résoudre le problème…
   Jérémie : Oui, c’est tout ce qui concerne le process, l’organisation.
   La méthodologie si tu préfères.
   Victor : Et la dernière catégorie ?
   Jérémie : Les conversations à propos de toutes ces conversations.
   Daniel : Oui, en quoi ça consiste ?
   Jérémie : Eh bien, par exemple, à essayer de comprendre pourquoi une
   conversation qui aurait dû être plus efficace par exemple, ne l’a pas
   été, et ce qu’on peut faire pour qu’elle le devienne.
   Daniel : Et donc, comme ça, vous savez, à tout moment, quel type de
   conversation vous êtes en train d’avoir ? C’est hilarant.
   Jérémie : Mais non, pas du tout.
   Daniel : Alors pourquoi tu me parles de toutes ces catégories Jérémie ?
   Jérémie : Tu m’as demandé quels types de conversation on tient. J’ai
   essayé de répondre à cette question.
   Daniel : Ha ! Tu es trop sérieux, Jérémie. Tiens bois encore un peu de
   champagne.
   Toi : Il se fait tard. Je rentre. A demain !

   (à suivre)
   Episodes Précédents :
   1 — Si le code pouvait parler
   2 — Voir / Avancer
   3 — Communication Breakdown
   4 — Driver / Navigator
   5 — Brown Bag Lunch
   6 — Conseils à emporter
   7 — Crise / Opportunité
   8 — Le Cinquième Étage
   9 — Que faire ?
   10 — Soit… Soit…
   11 — Boîtes et Flêches
   12 — Le prochain Copil
   13 — La Faille
   14 — Poussière
   15 — L’hypothèse et la Règle
   16 – Déplacements
   17 — Jouer et ranger
   18 — Arrangements
   19 — Mise au point
   20 — Expérimentation
   21 — Échantillons
   22 — Non-conclusions
   23 — Non-décisions
   24 — Épisode neigeux
   25 — Fusions et confusions
   26 — Débarquement
   27 — Tempête
   28 — Embardée
   29 — Aménagement
   30 — Interruptions
   31 — Normalisation
   32 — Outsiders
   33 — Fabrication
   34 — Observation
   35 — Perturbations
   36 — Conclusions
   37 — Nouvelle Donne
   38 — Transaction
   39 — Mutation
   40 — Exclusion Mutuelle
   41 — Préemption
   42 — Démonstration
   43 — Conversation
   44 — Exception
   45 — Explications
   46 — Télescopage
   47 — Négociations
   48 — Plaques tournantes
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans Software Craftsmanship.

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

   Sécurité

Comment conserver les mots de passe de ses utilisateurs en 2019 ?

   Posté le 02/10/2019 par Fabien Leite
   Mot de passe

   Lorsque vous concevez une application, vous vous posez forcément la
   question de l’authentification et du contrôle d’accès. Pour ça,
   plusieurs méthodes sont disponibles et la première qui vient
   généralement à l’esprit est l’utilisation d’un couple identifiant / mot
   de passe. Dans la mesure du possible, on préfèrera utiliser une
   solution dédiée à l’authentification et au contrôle d’accès : en bref,
   utiliser une solution d’IAM pour gérer ces aspects à votre place. C’est
   généralement plus simple à maintenir et c’est surtout souvent meilleur
   pour l’expérience utilisateur. …
   Lire la suite

   Méthode

Amélioration continue : Comment rester dynamique à mesure que l’équipe
s’agrandit ?

   Posté le 01/10/2019 par Etienne Girot

   Différentes études1 soutiennent qu’une équipe performante est une
   équipe qui est capable de remettre fréquemment en question ses modes de
   fonctionnement afin d’apprendre et s’améliorer en continu. Pour y
   arriver, elle : favorise l'émergence de nouvelles idées a moyen de
   valider ou d’invalider efficacement la pertinence de ces nouvelles
   idées est en mesure d’aligner ses membres derrière les idées retenues
   comme étant pertinentes Or, plus une équipe grandit (aussi bien en
   nombre de membres qu’en temps passé à travailler ensemble) plus elle
   est sujette à…
   Lire la suite

   Méthode innovation

Culture Innov’ : Quel ROI attendu ?

   Posté le 01/10/2019 par Sylvain Fagnent, Matthieu VETTER
   [school.png]

   Après des années et des millions investis sous la menace de la
   disruption, les Directions reviennent à une logique plus ROIste. Ce
   mouvement est sain pour optimiser les ressources rares et donner du
   sens au travail des innovateurs. Le risque est cependant de tuer dans
   l’oeuf des pépites potentielles en pilotant l’innovation comme un
   business opérationnel. En fonction des objectifs, les retours sur
   investissement (ROI) d’une démarche d’innovation seront différents. Et
   dans un monde de plus en plus focalisé sur la rentabilité à court
   terme,…
   Lire la suite

   Data Science

Ouvrir la boîte noire et comprendre les décisions des algorithmes

   Posté le 30/09/2019 par Annabelle Blangero
   [school.png]

   L’usage des algorithmes de traitement de données – de la simple requête
   SQL aux puissants algorithmes de recommandation et de personnalisation
   des géants de la Tech – s’est popularisé ces dernières années,
   notamment pour des utilisateurs traditionnellement hors du domaine IT.
   Cet usage se retrouve dans tous les secteurs (industrie, éducation,
   santé, sécurité, etc.) et tend à déléguer de plus en plus de décisions
   à des systèmes automatisés. Cette appropriation par le plus grand
   nombre rend les naufrages encore plus probables, et l’exemple de
   Cambridge…
   Lire la suite

   Archi & techno

Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos valeurs et le
garant de la vision technique d’OCTO.”

   Posté le 27/09/2019 par Joy Boswell
   [archi.png]

   Chez OCTO depuis plus de 10 ans , Meriem fait partie des personnes
   fondatrices de l’entreprise. Ancienne leadeuse de la tribu Nouvelles
   Architectures de Données, elle est désormais CTO et participe à la
   définition de la vision stratégique et technique d’OCTO. Qui de mieux
   pour nous parler du “tech leading à la OCTO” ?
   Lire la suite

   Data Science

Mise en application de DVC sur un projet de Machine Learning

   Posté le 27/09/2019 par Nicolas TOUSSAINT, Jérémy Bouhi
   [school.png]

   Introduction DVC (Data Version Control) est un package Python qui
   permet de gérer plus facilement ses projets de Data science. Cet outil
   est une extension de Git pour le Machine Learning, comme l’énonce son
   principal contributeur Dmitry Petrov dans cette présentation. DVC est à
   la fois comparable et complémentaire à Git. Il va s’occuper de
   synchroniser vos données et votre code. Il est donc particulièrement
   intéressant dans le cadre d’un projet de Machine Learning où le modèle
   et les données évoluent au fil du développement.…
   Lire la suite

   Archi & techno

BD – Le Déploiement Continu (CD)

   Posté le 26/09/2019 par Aryana Peze
   [archi.png]

   Hello ! Lors de la BD précédente, nous avons abordé le sujet de la CI
   (Intégration Continue). Et impossible de parler de CI sans parler de CD
   (Déploiement Continu)! En thoérie, la CD implique un déploiement
   automatique et quasi-systématique de chaque modification du code sur
   l'environnement de production. Les mises en production sont régulières
   et ne sont plus une source de stresse, et l'environnement de production
   est ainsi toujours à jour. En pratique, c'est un objectif très
   compliqué à atteindre, et pas toujours adapté. (Petite parenthèse…
   Lire la suite

   Agile

D’étudiant à mentor : rencontre avec notre Octo Thomas Le Flohic

   Posté le 24/09/2019 par Céline Audibert
   [agile.png]

   Thomas a fait un véritable parcours “à la OCTO” : après son école
   d’ingé, il rentre en stage au sein de notre tribu VIBE (Virtual
   Immersion and Bot Experience) et rejoint définitivement l’entreprise en
   intégrant le programme Skool. Un de ses profs à l’école était un Octo,
   Fabien. C’est ce qui lui a donné envie de venir frapper à notre porte.
   Comme lui, Thomas a voulu garder un lien avec l’école et transmettre
   son savoir. C’est ainsi qu’il s’est lancé dans l’accompagnement d’un
   projet de…
   Lire la suite

   Archi & techno

Interview Céline Gilet – « Le Tech Lead n’est pas un super héros ! »

   Posté le 23/09/2019 par Céline Audibert
   [archi.png]

   Depuis plus de 4 ans chez OCTO, Céline, membre de la tribu CRAFT, est
   devenue une référence parmi nos Tech Lead. Découvrez sa vision de ce
   rôle à part. Pour toi, quel est le rôle du Tech Lead ?  Pour moi, c’est
   faire en sorte que l’équipe au sens large (Développeurs, Ops,
   Fonctionnels, Product Owner) arrive à délivrer régulièrement de la
   valeur. Concrètement, il s’agit de jongler et prioriser en permanence
   entre plusieurs casquettes : expertise, accompagnement, coaching et
   formation.
   Lire la suite

   Méthode innovation

Injonctions paradoxales : un MVP … mais pour tous !

   Posté le 20/09/2019 par Dominique Lequepeys, Sylvain Fagnent
   un MVP pour tous

   Un MVP ... mais pour tout le monde ! Et si vous vouliez l’entendre
   cette injonction ? Mise en scène, écoutez la. Les racines du paradoxe
   D'un côté, les managers sont sous la pression du timing : ils sont
   séduits par le concept de MVP, Minimum Viable Product, présenté comme
   un moyen d'accélérer la mise sur le marché. D'un autre, ils sont sous
   la pression du chiffre : ils ont du mal à accepter qu'on se prive d'une
   partie du marché potentiel. En outre, ils…
   Lire la suite
   1234>
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Feed OCTO Talks ! » Comments Feed OCTO Talks
   ! » A chat with Doug Cutting about Hadoop Comments Feed Geo localizing
   Medline citations Serverless real-time architecture on AWS: there is a
   way ! alternate alternate

     * fr
     * pt-br

   OCTO Talks !

   Go to content
     * Archi & Techno
     * Methodology
     * Big Data
     * News

A chat with Doug Cutting about Hadoop

   Publication date 14/01/2016 by Nelly Grellier
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   We had the chance to interview Doug Cutting during the Cloudera
   Sessions in Paris, October 2014. Doug is the creator behind Hadoop and
   Cloudera’s Chief Architect. Here is our exchange below:

   DougBWSquare

A question is: how does it feel to see that Hadoop is actually becoming the
must have, the default way of storing and computing over data in large
enterprise companies?

   Rationally it feels very good. It’s a technology that’s supposed to do
   that. Emotionally it’s very satisfying, but also I must say I must be
   very lucky. I was in the right place at the right time and happened to
   be the person. Someone else would have done this had I not, by now.


   Download our white paper “Hadoop Roadmap”

It’s funny because yesterday you were mentioning how Google released that
paper about GFS and then about MapReduce, and you seemed surprised that no
one else has gone and implemented the paper. How would you describe this,
because it was a very big, big task that some people were daunted by taking
on or…?

   I think, again, I have the right experience from having put some work
   in open source. I worked on search engines and I could see the value in
   the technology, I understood the problem, and that combination. And I
   think I’ve also been in the software business long enough so that’s why
   I knew what it’d take to build a project that would be useful, that
   would be used. And I think no one else was positioned ready enough in
   the competition with that combination of properties. I’ve been able to
   take advantage of these papers and implement them as open source, and
   get them out to people. My guess, I don’t know. It wasn’t my plan.

Were you expecting that it would get take such a big, big impact?

   No, not at all.

OK, now I guess it’ll be more of a technical question: you mentioned
yesterday (I was there yesterday and today) that you know there are all these
tools that are coming out, like, building on top of Hadoop and bringing a new
technology and a new usage of data – how do you see Hadoop changing,
architecturally speaking, to be able to provide even more capabilities in the
future?

   It’s a very general architecture. It’s in many ways, much like I said,
   an operating system. An operating system, I’ve been showing, has
   storage, has a scheduler, has security mechanisms. Already the
   challenge is to support all kinds of different applications. So I think
   that the design it has right now is more or less sufficient to permit a
   very wide range.

Just like operating systems haven’t changed since Windows.

   Yeah, not fundamentally. Since the 60’s. Those basic capabilities give
   you a platform you can develop lots of different applications on that
   can share the hardware, in a sense. It’s really… Well, a Java OS is
   sort of “get out of the way” and let applications share the hardware.

Provide abstractions as Jim Baum said.

   Exactly.

To deal with complexity.

   And so I think that’s a role that Hadoop is filling more and more.

   I know it needs a radical re-architecture to do that. Whether people
   will implement alternate file systems…That might happen, we’ll see.

OK. Thank you so much. And do you see, all these tools, like, you see Kafka
for log aggregation across DC, and we see Storm for stream processing, and
all these things. Do you see new usages that haven’t come out yet for data?
You can search on it, you can index it, you can stream it and process it in
real time…

   We think there are lots of opportunities for more vertical applications
   in different industries that are very specific. Things that can process
   images, tools that can process data… There are lots of different areas
   where there aren’t tools today. Not to mention verticals like insurance
   and banking and so on. Some people see commercial offerings and some
   people see open source offerings. I think right now what people are
   seeing are more the lower-level tools that can be plugged. I think more
   and more, higher and higher upper stack will see open-source
   implementations commoditizing the value of the stack. That’s an ongoing
   process.

   WP_Hadoop_carto_3D


   Want to know more about Hadoop and the Hadoop ecosystem? Have a look at
   our Hadoop map (English version)!

   Our Hadoop White Paper and Roadmap are also available for free download
   (French only).
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   This entry was posted in Big Data, News.

Recent Posts

     * Seven shades of Git
     * Accelerating NiFi flows delivery: Part 1
     * Cache me if you can – 2
     * Android Material Components: Exploring MaterialShapeDrawable
     * Comic – Infrastructure as Code (IaC)

Un commentaire sur “A chat with Doug Cutting about Hadoop”

     Nick
   08/01/2018 à 09:59
   Link to Hadoop map is broken :(

Leave a Reply Cancel reply

   Your email address will not be published. Required fields are marked *

   Comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Name * ______________________________

   Email * ______________________________

   Website ______________________________
   (BUTTON) Leave a comment
   This form is protected by Google Recaptcha

   ____________________ (BUTTON)

   Search

   Incubated startups at OCTO
   Appaloosa, your company App Store

   Appaloosa, your company App Store
   www.laduckconf.com www.laduckconf.com

   Our white papers :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Who we are
          + Who we are
          + Locations
          + Our products
          + Partners
          + Investors
     * What we do
          + What we do
          + How we do it
          + Publications
          + Events
          + OCTO Academy
     * Join us
          + OCTO is hiring!
          + Inside OCTO
          + Our tribes
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Headquarters:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Legal mentions

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » Browser 2.0 : un nouveau browser pour des interactions
   plus riches Flux des commentaires Maven Community news – Août 2007 WWDC
   2007 alternate alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

Browser 2.0 : un nouveau browser pour des interactions plus riches

   Posté le 04/09/2007 par Andre Nedelcoux
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Lorsque l’on cherche à classifier les applications et que l’on
   s’intéresse aux technologies d’interface homme-machine, nous avons
   désormais pris l’habitude de distinguer deux filières principales : la
   filière RIA,  » Rich Internet Applications  » et la filière RDA,  »
   Rich Desktop Applications « . Est-il possible de tirer partie des
   avantages respectifs de ces deux technologies? Un petit délire sur une
   solution mixte…

   Dans le premier cas (RIA), le navigateur web est utilisé comme
   environnement d’exécution des applications, c’est-à-dire comme
   conteneur pour le code développé en HTML, Javascript et autre CSS.
   Grâce à des efforts importants de communautés open source et d’éditeur,
   le niveau d’interaction (de  » richesse « ) proposé par ce type
   d’applications est en augmentation constante, avec des efforts
   importants pour se rapprocher du niveau d’ergonomie des applications
   natives. On peut citer à titre d’exemple le framework ajax  » page bus
    » de Tibco qui a vocation à proposer une approche événementielle pour
   les développements Javascript.
   Dans le second cas (RDA), le navigateur web est utilisé comme moyen
   d’accès à l’application : un lien sur une page permet de lancer un
   plugin côté client qui, via le browser, récupère les composants de
   l’application. Une fois téléchargée, l’application s’exécute de manière
   autonome, le browser n’étant plus utilisé. Comme exemple concret, il
   est possible de citer la technologie Java Web Start de Sun qui joue le
   rôle de lanceur et d’updateur depuis le serveur (si une nouvelle
   version de l’application est disponible). Dans le monde Eclipse, le
   browser est même totalement contourné : c’est la plate-forme Eclipse
   qui se charge de récupérer les nouvelles versions de l’application et
   qui agit comme un  » browser d’applications RDA « . Il y a ici peu de
   limites dans le niveau d’ergonomie atteignable, sans les limites
   imposées par le browser (sécurité, accès au file system, accès à des
   périphériques…).
   Il peut être intéressant de réfléchir aux moyens de marier le meilleur
   de ces deux mondes. Ainsi, on peut imaginer un browser web capable
   d’offrir une vraie  » infrastructure  » desktop, contenant des
   applications web qui pourraient désormais interagir fortement avec ce
   browser, en le modifiant (par exemple par ajout de fenêtre, d’entrées
   de menus du browser…) ou en communiquant avec lui (par exemple par
   déclenchement d’un événement qui serait propagé à l’OS ou à une autre
   application également hébergée dans le browser). Le browser deviendrait
   un pont entre l’application web et les fonctions offertes par l’OS,
   puis dans un second temps avec toutes les applications du poste ; il
   serait capable de gérer le multi-fenêtrage associé à plusieurs
   applications ouvertes en parallèle, avec toutes les fonctionnalités
   associées (sauvegarde de l’état du browser et des préférences
   utilisateur, habilitations…).
   Que gagnerait-on avec ce browser hybride ? Il serait possible de lever
   des limites des deux filières classiques puisque l’on aurait une
   infrastructure totalement ouverte vers le poste local, qui permettrait
   aussi une ouverture sur le web. Il serait possible de proposer le même
   niveau d’interactivité que les applications locales sans avoir besoin
   de réinventer toute une ergonomie dans le cadre contraint du browser
   habituel. D’un point de vue performances et fluidité des applications,
   un gain substantiel pourrait être envisagé puisque le browser
   permettrait d’utiliser des composants performants de l’OS, dont
   l’imitation en RIA donne parfois lieu à des implémentations poussives.
   Fermez les yeux deux secondes et imaginez un peu :
     * L’utilisateur lance son browser  » new generation  » et saisit
       l’URL d’une application
     * Lors de son lancement, l’application  » déclare  » au browser
       qu’elle a besoin des 3 onglets supplémentaires (positionnés en haut
       à gauche pour 2 d’entre eux et en bas pour le dernier afin de
       constituer une barre de messages pour l’utilisateur) ainsi que d’un
       composant  » browser  » chargé de piloter MS Word
     * Durant l’exécution, l’application va pousser des informations dans
       chacun des 3 onglets, lancer MS Word pour lui injecter un document
       type rempli de données issues de l’application, que l’utilisateur
       modifie (dans MS Word !) et re-soumet à l’application
     * Parce qu’il a besoin de faire une recherche sur le web,
       l’utilisateur lance Google : il lui suffit de lancer la page web
       dans un onglet. Pour une application web classique, la plate-forme
       est un simple browser 1.0 et il héberge l’application de manière
       transparente.

   Dans une utilisation  » totale « , le browser deviendrait une
   plate-forme RDA capable d’héberger des applications web. Dans une
   utilisation  » basique « , on aurait un simple browser web. L’avantage
   est… qu’il est possible de faire les deux au sein d’une même
   plate-forme, selon le type d’application ! Par ailleurs, l’accès aux
   ressources et aux composants IHM natifs de l’OS serait possible via ce
   browser.
   Alors, vous êtes partants ? Comment pourrait-on implémenter avec les
   technologies d’aujourd’hui cette nouvelle filière de browser ?
   Première solution : en s’appuyant sur un browser qui offre déjà un
   modèle de framework de programmation d’IHM, à savoir FireFox avec son
   framework XUL. Notre application web pourrait être développée de
   manière  » classique  » (Javascript + HTML + CSS + tiers serveur en
   langage de votre choix) et envoyer des instructions XUL au browser
   (avec un mécanisme à trouver J) pour que celui-ci se déforme et ajoute
   des morceaux d’IHM  » réellement riches  » (= en technologie de l’OS).
   Deuxième solution : en s’appuyant sur une solution RDA capable de
   piloter un browser, à savoir Eclipse RCP par exemple. Dans ce modèle,
   l’application web classique remonte des instructions à la plate-forme
   qui se déforme également ; ce mécanisme est ici possible du fait de la
   capacité d’Eclipse RCP à intégrer (via un composant ActiveX) un browser
   web et à communiquer avec lui via ce  » pont  » ActiveX. Autre avantage
   : il est également possible de déployer dans ce browser de vraies
   applications RDA, développées dans la technologie Eclipse.
   De nombreux enjeux restent à adresser pour parvenir à développer ce
   type d’applications :
     * Comment développer une application  » cross browser  » ? Pour
       l’être vraiment, elles n’exploiteraient pas les nouvelles
       fonctionnalités offertes par ce browser de nouvelle génération et
       seraient alors de  » simples applications internet riches « …
     * Faut-il introduire une nouvelle technologie, c’est-à-dire encore un
       autre browser en plus du nombre déjà important ? Comment
       capitaliser sur le très large parc installé d’IE?
     * Quel langage de programmation pour l’API du browser ? Quel
       protocole derrière cette API, c’est-à-dire quelle structure de
       message et d’échange de message entre les applications et le
       browser hybride ?
     * Comment gérer les problématiques de sécurité de ce nouveau browser
       qui permet à une application web d’interagir fortement avec l’OS ?
     * Enfin, la complexité introduite en vaut-elle la chandelle ? On peut
       imaginer que développer une telle application pour un développeur
       peu expérimenté risque d’être assez complexe, vu le mélange de
       technologies.

   … mais on peut toujours rêver :o). Il y a quelques années, on avait des
   écrans noirs et verts en 32/70.
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans Brèves de consultants et taggué RIA.

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

3 commentaires sur “Browser 2.0 : un nouveau browser pour des interactions
plus riches”

     ben
   04/09/2007 à 21:24

   Que pense tu de XAML?
   www.xaml.fr/

     tvi
   08/09/2007 à 21:42

   La 2ème solution présente l'inconvénient d'utiliser un truc très très
   propriétaire (ActiveX) entre 2 morceaux très ouverts (java avec Eclipse
   RCP et HTML/CSS), et rend le conteneur dépendant du browser puisqu'il
   n'y a pas d'interface COM unique (qu'implémenteraient IE, Mozilla,
   etc.).
   Ca va bien pour des applis intranet en entreprise (j'ai un exemple en
   tête ;-) mais au-delà... couic.

     Dominique De Vito
   22/09/2007 à 18:06

   D'après ce que j'ai compris de la voie suivie par Adobe, le navigateur
   Air (ex-Apollo) est le navigateur hybride envisagé ici, ou en tout cas,
   en est très proche de part sa capacité à se comporter comme un
   navigateur normal et sa facilité à pouvoir, par ex, accéder aux
   ressources de la machine locale. Ce navigateur d'Adobe serait plutôt du
   premier type détaillé plus haut.

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha

   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate OCTO Talks ! » Flux OCTO Talks ! » Flux des commentaires
   OCTO Talks ! » Règles de qualité de code Flux des commentaires
   alternate alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

Règles de qualité de code

   Posté le 21/06/2011 par Maxime ARNSTAMM
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Règles de qualité de code Sonar .NET
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans .

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
   #alternate alternate alternate OCTO Talks ! » Flux OCTO Talks ! » Flux
   des commentaires OCTO Talks ! » query Flux des commentaires alternate
   alternate

     * en
     * pt-br

   OCTO Talks !

   Aller au contenu de la page
     * Stratégie SI
     * archi & techno
     * Méthode
     * Digitalisation
     * Big Data
     * Évènement

query

   Posté le 20/02/2019 par Joy Boswell
     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

     * Tweet
     * Share 0
     * +1
     * LinkedIn 0

   Cet article a été posté dans .

Articles récents

     * Comment conserver les mots de passe de ses utilisateurs en 2019 ?
     * Amélioration continue : Comment rester dynamique à mesure que
       l’équipe s’agrandit ?
     * Culture Innov’ : Quel ROI attendu ?
     * Ouvrir la boîte noire et comprendre les décisions des algorithmes
     * Meriem Berkane, CTO : “Le Tech Lead est l’incarnation de nos
       valeurs et le garant de la vision technique d’OCTO.”

Laisser un commentaire Annuler la réponse

   Votre adresse de messagerie ne sera pas publiée. Les champs
   obligatoires sont indiqués avec *

   Commentaire
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   [ ] Me notifier par mail en cas de nouveaux commentaires

   Nom * ______________________________

   Adresse de messagerie * ______________________________

   Site web ______________________________
   (BUTTON) Laisser un commentaire
   Ce formulaire est protégé par Google Recaptcha
   ____________________ (BUTTON)

   Chercher

   Les start-ups incubées chez OCTO :
   Appaloosa, App Store d’entreprise

   Les prochaines formations :
    1. [R]évolution Blockchain
    2. Théorie U
    3. AWS : Notions techniques Amazon Web Services de base
    4. Administrer la plateforme Hadoop 2.X Hortonworks : fondamentaux
    5. Qualité des développements avec Test Driven Development

   Appaloosa, App Store d’entreprise

   Suivez l’aventure sur leur blog
   www.laduckconf.com www.laduckconf.com

   Nos livres blancs :
   Culture DevOps 2 Culture DevOps 2 Guide de survie dans la jungle
   technologique Culture Code Culture Code Software Craftsmanship : Better
   places with better code Roadmap Produit Roadmap Produit Et si elle
   devenait une direction plutôt qu’un plan établi ?

     * Nous connaître
          + Pourquoi OCTO ?
          + Où trouver OCTO ?
          + Nos produits
          + Nos partenaires
          + Investisseurs
     * Notre mission
          + Ce que nous faisons
          + Comment nous le faisons
          + Publications
          + Évènements
          + OCTO Academy
     * Nous rejoindre
          + OCTO Recrute !
          + Découvrez OCTO de l'intérieur
          + Nos tribus
     * International
          + Paris
          + Rabat
          + Lausanne
          + Sydney

   OCTO Technology
   Part of Accenture Digital
     * Paris
     * Rabat
     * Lausanne
     * Sydney


     * Siège:
     * 34 avenue de l'Opéra,
     * 75002 Paris,
     * France
     * +33 (0)1 58 56 10 00

     * Contact
     * Mentions legales

   En navigant sur ce site, vous acceptez l’utilisation de cookies ou
   autres traceurs vous permettant une utilisation optimale du site
   (partages sur les réseaux sociaux, statistiques de visite,
   etc.)J'accepte
